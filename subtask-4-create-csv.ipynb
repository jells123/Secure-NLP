{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZaobserwowane błędy:\\n- jeden token nie ma żadnej annotation\\n- występują po dwie relacje które opisują dokładnie to samo, a mają różne ID - duplikaty\\n- 17 tokenów nie zawiera żadnych relacji - ? sprawdzić ? - kilka takich problemów w obrębie jednego pliku\\n- gdyby jednak subject->action, to dwóm brakuje relacji\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Zaobserwowane błędy:\n",
    "- jeden token nie ma żadnej annotation\n",
    "- występują po dwie relacje które opisują dokładnie to samo, a mają różne ID - duplikaty\n",
    "- 17 tokenów nie zawiera żadnych relacji - ? sprawdzić ? - kilka takich problemów w obrębie jednego pliku\n",
    "- gdyby jednak subject->action, to dwóm brakuje relacji\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jells123/Documents/ENGINEER/DATA/1-Raw data/data/'\n",
    "ann = '/annotations'\n",
    "\n",
    "sets = {\n",
    "    'Train' : 'train',\n",
    "    'Dev' : 'dev',\n",
    "    'Test' : 'test_3'\n",
    "}\n",
    "set_type = 'Test'\n",
    "path = root + sets[set_type] + ann\n",
    "\n",
    "rel_pattern = r'(?P<r_id>R[0-9]+)\\s+[A-Za-z]+\\s+(?P<r_first>[A-Za-z]+:T[0-9]+)\\s+(?P<r_second>[A-Za-z]+:T[0-9]+)'\n",
    "ann_pattern = r'(?P<a_id>A[0-9]+)\\s+(?P<cat>[A-Za-z]+)\\s+(?P<t_id>T[0-9]+)\\s+(?P<text>[^\\n]+)'\n",
    "token_pattern = r'(?P<t_id>T[0-9]+)\\s+(?P<t_type>[A-Z][a-z]+) (?P<indices>[0-9]+ [0-9]+[^\\t]+)\\t(?P<t_text>.+)\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cats = ['ActionName', 'Capability', 'StrategicObjectives', 'TacticalObjectives']\n",
    "# for each main category, store token groups that wasn't annotated using this main category\n",
    "\n",
    "def init_data():\n",
    "    data = dict([(cat, {}) for cat in main_cats])\n",
    "    data['ActionName'] = {\n",
    "        'File' : [],\n",
    "        'Network' : [],\n",
    "        'Other' : []\n",
    "    }\n",
    "    data['Capability'] = {\n",
    "        'command_and_control' : [],\n",
    "        'infection/propagation' : [],\n",
    "        'other' : []\n",
    "    }\n",
    "    undefined = dict([(cat, []) for cat in main_cats])\n",
    "    return data, undefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation(anns, words, token, cat):\n",
    "    \n",
    "    # say cat == 'ActionName'\n",
    "    action_name = list(filter(lambda x : x[1] == cat, anns))\n",
    "    if action_name:\n",
    "        m = re.search(r'(?P<id>[0-9]{3}):(?P<type>[A-Za-z]+)-(?P<name>.+)\\s*', action_name[0][3])\n",
    "        typ, name = m.group('type'), m.group('name')\n",
    "        \n",
    "        if cat == 'Capability':\n",
    "            # special treatment :)\n",
    "            if name in data[cat].keys():\n",
    "                data[cat][name].append(words)\n",
    "            else:\n",
    "                data[cat]['other'].append(words)\n",
    "        else:\n",
    "            if typ in data[cat].keys():\n",
    "                data[cat][typ].append(words)\n",
    "            else:\n",
    "                data[cat]['Other'].append(words)\n",
    "            \n",
    "    else:\n",
    "        undefined[cat].append(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_relation(t, tokens, rels, anns, cat):\n",
    "    \n",
    "    global tokens_count\n",
    "    tokens_count += 1\n",
    "    \n",
    "    relations = list(filter(lambda x : x[1] == 'Action:'+t[0] , rels))\n",
    "    if relations:\n",
    "        mod_relations = list(filter(lambda x : 'Modifier' in x[2], relations))\n",
    "        if mod_relations:\n",
    "            for mr in mod_relations:\n",
    "                relations += list(filter(lambda x : x[1] == mr[2], rels))\n",
    "    \n",
    "    if include_subject:\n",
    "        subject = list(filter(lambda x : 'Subject' in x[1] and x[2] == 'Action:'+t[0], rels))\n",
    "        if subject:\n",
    "            subject = subject[0]\n",
    "            relations.append(subject)\n",
    "\n",
    "    if not relations:\n",
    "        print(\"Token {} has no relation.\".format(t))\n",
    "        global relation_missing\n",
    "        relation_missing += 1\n",
    "    \n",
    "    words = []\n",
    "    words.append(t[3])\n",
    "    \n",
    "    for r in relations:\n",
    "        \n",
    "        if include_subject and 'Subject' in r[1]: \n",
    "            # subject -> action\n",
    "            token_id = r[1].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                words.insert(0, token_word[0][3])\n",
    "            \n",
    "        else:\n",
    "            # action -> object, action -> mod -> object\n",
    "            token_id = r[2].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                if token_word[0][3][0].isupper():\n",
    "                    words.insert(0, token_word[0][3])\n",
    "                else:\n",
    "                    words.append(token_word[0][3])\n",
    "    \n",
    "    # relation gathered and here we create the dataset\n",
    "    # id prefer it in anotehr function but whatever\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    process_annotation(annotations, words, t, cat)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_coords(t, text, off=0):\n",
    "    indices = t[2]\n",
    "    indices = indices.split(';') if ';' in indices else [indices]\n",
    "    real_token = ''\n",
    "    indices = [[int(idx) for idx in i.split()] for i in indices]\n",
    "    for i in indices:\n",
    "        real_token += text[i[0]+off : i[1]+off] + ' '\n",
    "        \n",
    "    while '- ' in real_token:\n",
    "        real_token = real_token.replace('- ', '')\n",
    "    return real_token.strip(), (indices[0][0]+off, indices[-1][1]+off)\n",
    "\n",
    "def get_token_neighbors(token, coords, text, neighbors):\n",
    "    \n",
    "    left = (neighbors-1)/2\n",
    "    right = left\n",
    "    \n",
    "    l_idx = coords[0] - 1\n",
    "    r_idx = coords[1] + 1\n",
    "    \n",
    "    words = []\n",
    "    while left:\n",
    "        while text[l_idx].isspace():\n",
    "            l_idx -= 1\n",
    "        word = ''\n",
    "        while not text[l_idx].isspace():\n",
    "            word += text[l_idx]\n",
    "            l_idx -= 1\n",
    "        \n",
    "        word = word[::-1]\n",
    "        if word[0] == '<' or word[-1] == '>': #html tag\n",
    "            l_idx -=1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.insert(0, word)\n",
    "            left -= 1\n",
    "    \n",
    "    words.append(token)\n",
    "    while right:\n",
    "        while text[r_idx].isspace():\n",
    "            r_idx += 1\n",
    "        word = ''\n",
    "        while not text[r_idx].isspace():\n",
    "            word += text[r_idx]\n",
    "            r_idx += 1\n",
    "            \n",
    "        if word[0] == '<' or word[-1] == '>':\n",
    "            r_idx += 1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.append(word)\n",
    "            right -= 1\n",
    "            \n",
    "    return words\n",
    "        \n",
    "def gather_data(words, anns, t, cat):\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    process_annotation(annotations, words, t, cat)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ActionName': {'File': [], 'Network': [], 'Other': []}, 'Capability': {'command_and_control': [], 'infection/propagation': [], 'other': []}, 'StrategicObjectives': {}, 'TacticalObjectives': {}}\n",
      "\n",
      ">>> butterfly-corporate-spies-out-for-financial-gain.ann\n",
      "\n",
      ">>> Equation_group_questions_and_answers.ann\n",
      "\n",
      ">>> Dissecting-LinuxMoose.ann\n",
      "Token ('T136', 'Action', '52031 52036', 'reach') has no annotation.\n",
      "\n",
      ">>> Dissecting-the-Kraken.ann\n",
      "Token ('T16', 'Action', '10699 10705', 'update') has no relation.\n",
      "Token ('T17', 'Action', '10710 10719', 'uninstall') has no relation.\n",
      "\n",
      ">>> DEEP_PANDA_Sakula.ann\n"
     ]
    }
   ],
   "source": [
    "data, undefined = init_data()\n",
    "print(data)\n",
    "\n",
    "relation_mode = True\n",
    "cat = 'Capability'\n",
    "\n",
    "relation_missing = 0\n",
    "annotation_missing = 0\n",
    "tokens_count = 0\n",
    "\n",
    "include_subject = True\n",
    "action_tokens = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    if filename.endswith('.ann'):\n",
    "        print(\"\\n>>> {}\".format(filename))\n",
    "        with open(path + '/' + filename) as ann_file:\n",
    "            content = ann_file.read()\n",
    "            \n",
    "            # match lines with a token            \n",
    "            tokens = re.findall(token_pattern, content)\n",
    "            relations = re.findall(rel_pattern, content)\n",
    "            annotations = re.findall(ann_pattern, content)\n",
    "            \n",
    "            if not relation_mode:\n",
    "            # read both .ann and .txt file\n",
    "                with open(path + \"/\" + filename.replace(\".ann\", \".txt\")) as txt_file:\n",
    "                    text = txt_file.read()\n",
    "                    offset = 0\n",
    "                    for t in tokens:\n",
    "                        if t[1] == 'Action':\n",
    "                            # get token from annotations + text and compare them, are they equal?\n",
    "                            ann_token = t[3]\n",
    "                            tokens_count += 1\n",
    "                            while '- ' in ann_token:\n",
    "                                ann_token = ann_token.replace('- ', '')\n",
    "\n",
    "                            idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "                            # oops, we got a mismatch!\n",
    "                            if ann_token != idx_token:\n",
    "                                # probably something went wrong! - we need to deal with it as always :)\n",
    "                                where_is = text.find(ann_token)\n",
    "                                offset = where_is - int(t[2].split()[0])\n",
    "                                idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "\n",
    "                            words = get_token_neighbors(ann_token, coords, text, 9)\n",
    "                            gather_data(words, annotations, t, cat)\n",
    "                            \n",
    "            else:\n",
    "            # older version -> finds token groups by relation\n",
    "                for t in tokens:\n",
    "                    if t[1] == 'Action':\n",
    "                        get_token_words_relation(t, tokens, relations, annotations, cat)         \n",
    "                        action_tokens.append(t[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-File    829\n",
      "File       325\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "no-Network    781\n",
      "Network       373\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "no-Other    698\n",
      "Other       456\n",
      "Name: label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "For main category\n",
    "and its sub categories!\n",
    "'''\n",
    "\n",
    "main_cat = cat\n",
    "include_undefined = False\n",
    "\n",
    "for sub_cat in data[main_cat].keys():\n",
    "\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for cat in data[main_cat].keys():\n",
    "        for sent_set in data[main_cat][cat]:\n",
    "\n",
    "            sentence = ' '.join(sent_set)\n",
    "\n",
    "            # sentence processing here\n",
    "            for punct in [\"“\", \"”\", \"\\[\", \"\\]\"]:\n",
    "                sentence = re.sub(punct, \"\", sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    #         labels.append(main_cat)\n",
    "\n",
    "            if cat == sub_cat:\n",
    "                labels.append(sub_cat)\n",
    "            else:\n",
    "                labels.append('no-' + sub_cat)\n",
    "\n",
    "    if include_undefined:\n",
    "        for sent_set in undefined[main_cat]:\n",
    "            sentences.append(\" \".join(sent_set))\n",
    "            labels.append('no-' + sub_cat)\n",
    "\n",
    "    sent_label = 'text-rel' if relation_mode else 'text-neigh'\n",
    "            \n",
    "    if sub_cat not in temp_datasets:\n",
    "        dataset = pd.DataFrame({\"label\" : labels, sent_label : sentences})\n",
    "        dataset['label_num'] = dataset.label.map({'no-'+sub_cat:0, sub_cat:1})\n",
    "        temp_datasets[sub_cat] = dataset\n",
    "        print(dataset.label.value_counts(), \"\\n\")\n",
    "    else:\n",
    "        print(len(sentences))\n",
    "        temp_datasets[sub_cat][sent_label] = sentences\n",
    "        print(temp_datasets[sub_cat].label.value_counts(), \"\\n\")\n",
    "\n",
    "temp_datasets[sub_cat].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_cat in temp_datasets.keys():\n",
    "    dset = temp_datasets[sub_cat]\n",
    "    dset.to_csv('{}-{}.csv'.format(sub_cat.replace('/', '_'), set_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "110\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text-neigh</th>\n",
       "      <th>label_num</th>\n",
       "      <th>text-rel</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capability</td>\n",
       "      <td>a command-and-control (C&amp;C) server used in an ...</td>\n",
       "      <td>1</td>\n",
       "      <td>used a command-and-control (C&amp;C) server in an ...</td>\n",
       "      <td>obtaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capability</td>\n",
       "      <td>and went on to create a variant of Backdoor.Ji...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bda9.tmp create a variant of Backdoor.Jiripbot...</td>\n",
       "      <td>eavesdropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capability</td>\n",
       "      <td>air-gapped networks, they can save these comma...</td>\n",
       "      <td>1</td>\n",
       "      <td>they save these commands in the hidden area of...</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capability</td>\n",
       "      <td>overview Linux/Moose will periodically communi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Linux/Moose communicate with a set of command ...</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capability</td>\n",
       "      <td>configuration C&amp;C server, will provide configu...</td>\n",
       "      <td>1</td>\n",
       "      <td>the configuration C&amp;C server provide configura...</td>\n",
       "      <td>executed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                         text-neigh  label_num  \\\n",
       "0  Capability  a command-and-control (C&C) server used in an ...          1   \n",
       "1  Capability  and went on to create a variant of Backdoor.Ji...          1   \n",
       "2  Capability  air-gapped networks, they can save these comma...          1   \n",
       "3  Capability  overview Linux/Moose will periodically communi...          1   \n",
       "4  Capability  configuration C&C server, will provide configu...          1   \n",
       "\n",
       "                                            text-rel         token  \n",
       "0  used a command-and-control (C&C) server in an ...     obtaining  \n",
       "1  Bda9.tmp create a variant of Backdoor.Jiripbot...  eavesdropped  \n",
       "2  they save these commands in the hidden area of...        insert  \n",
       "3  Linux/Moose communicate with a set of command ...          used  \n",
       "4  the configuration C&C server provide configura...      executed  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "For main category\n",
    "and that's all!\n",
    "'''\n",
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "main_cat = cat\n",
    "include_undefined = False\n",
    "\n",
    "for sub_cat in data[main_cat].keys():\n",
    "    tokens_list = data[main_cat][sub_cat]\n",
    "    for tokens in tokens_list:\n",
    "        sentence = ' '.join(tokens)\n",
    "\n",
    "        # sentence processing here\n",
    "        for punct in [\"“\", \"”\", \"\\[\", \"\\]\"]:\n",
    "            sentence = re.sub(punct, \"\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        labels.append(main_cat)\n",
    "\n",
    "for sent_set in undefined[main_cat]:\n",
    "    sentence = ' '.join(sent_set)\n",
    "    for punct in [\"“\", \"”\", \"\\[\", \"\\]\"]:\n",
    "        sentence = re.sub(punct, \"\", sentence)\n",
    "        \n",
    "    sentences.append(sentence)\n",
    "    labels.append('No' + main_cat)\n",
    "   \n",
    "c_label = 'text-rel' if relation_mode else 'text-neigh'\n",
    "if dataset is None:\n",
    "    dataset = pd.DataFrame({\"label\" : labels, c_label : sentences})\n",
    "    dataset['label_num'] = dataset.label.map({'No'+main_cat:0, main_cat:1})\n",
    "else:\n",
    "    dataset[c_label] = sentences\n",
    "\n",
    "if relation_mode:\n",
    "    print(len(undefined[main_cat]))\n",
    "    print(len(action_tokens))\n",
    "    print()\n",
    "    dataset['token'] = action_tokens\n",
    "    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capability      88\n",
      "NoCapability    22\n",
      "Name: label, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset.to_csv('neighbours/{}-{}.csv'.format(main_cat, set_type))\n",
    "dataset.to_csv('{}-{}.csv'.format(main_cat, set_type))\n",
    "print(dataset.label.value_counts(), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
