{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis file gathers all previously selected categories:\\n1. ActionName\\n    File\\n    Network\\n    Other\\n2. Capability\\n    infection_propagation\\n    ...something\\n    other\\ninto one DataFrame, instead of gathering binary data into files \\n(for example ActionName - NoActionName)\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This file gathers all previously selected categories:\n",
    "1. ActionName\n",
    "    File\n",
    "    Network\n",
    "    Other\n",
    "2. Capability\n",
    "    infection_propagation\n",
    "    ...something\n",
    "    other\n",
    "into one DataFrame, instead of gathering binary data into files \n",
    "(for example ActionName - NoActionName)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jells123/Documents/ENGINEER/DATA/1-Raw data/data/'\n",
    "ann = '/annotations'\n",
    "\n",
    "sets = {\n",
    "    'Train' : 'train',\n",
    "    'Dev' : 'dev',\n",
    "    'Test' : 'test_3'\n",
    "}\n",
    "\n",
    "rel_pattern = r'(?P<r_id>R[0-9]+)\\s+[A-Za-z]+\\s+(?P<r_first>[A-Za-z]+:T[0-9]+)\\s+(?P<r_second>[A-Za-z]+:T[0-9]+)'\n",
    "ann_pattern = r'(?P<a_id>A[0-9]+)\\s+(?P<cat>[A-Za-z]+)\\s+(?P<t_id>T[0-9]+)\\s+(?P<text>[^\\n]+)'\n",
    "token_pattern = r'(?P<t_id>T[0-9]+)\\s+(?P<t_type>[A-Z][a-z]+) (?P<indices>[0-9]+ [0-9]+[^\\t]+)\\t(?P<t_text>.+)\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data():\n",
    "    data = {\n",
    "        'ActionName' : [],\n",
    "        'Capability' : [],\n",
    "        \n",
    "        'token' : [],\n",
    "        'text-rel-subj' : [],\n",
    "        'text-rel' : [],\n",
    "        'text-neigh' : []\n",
    "    }\n",
    "    undefined = None\n",
    "    return data, undefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation(anns, words, token):\n",
    "    \n",
    "#     data['Text'].append(words)\n",
    "    \n",
    "    current_annotations = {}\n",
    "    for ann in anns:\n",
    "        m = re.search(r'(?P<id>[0-9]{3}):(?P<type>[A-Za-z]+)-(?P<name>.+)\\s*', ann[3])\n",
    "        typ, name = m.group('type'), m.group('name')\n",
    "        typ, name = re.sub(r'\\/', '_', typ), re.sub(r'\\/', '_', name)\n",
    "        main_cat = ann[1]\n",
    "        if main_cat == 'Capability':\n",
    "            # special treatment for Capability because there are no sub-categories\n",
    "            current_annotations[main_cat] = name\n",
    "        else:\n",
    "            current_annotations[main_cat] = typ\n",
    "    \n",
    "    ann_result = []\n",
    "    for c in [\n",
    "        ['ActionName', ['File', 'Network']], \n",
    "        ['Capability', ['infection_propagation', 'command_and_control']]\n",
    "    ]:\n",
    "        category = c[0]\n",
    "        sub_cats = c[1]\n",
    "        if category in current_annotations.keys():\n",
    "            sub_cat = current_annotations[category]\n",
    "            if sub_cat in sub_cats:\n",
    "                ann_result.append(sub_cat)\n",
    "            else:\n",
    "                ann_result.append(\"Other\")\n",
    "        else:\n",
    "            ann_result.append(\"-\")\n",
    "            \n",
    "    return ann_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_relation(t, tokens, rels, anns, include_subject=False):\n",
    "    \n",
    "    global tokens_count\n",
    "    tokens_count += 1\n",
    "    \n",
    "    relations = list(filter(lambda x : x[1] == 'Action:'+t[0] , rels))\n",
    "    if relations:\n",
    "        mod_relations = list(filter(lambda x : 'Modifier' in x[2], relations))\n",
    "        if mod_relations:\n",
    "            for mr in mod_relations:\n",
    "                relations += list(filter(lambda x : x[1] == mr[2], rels))\n",
    "    \n",
    "    if include_subject:\n",
    "        subject = list(filter(lambda x : 'Subject' in x[1] and x[2] == 'Action:'+t[0], rels))\n",
    "        if subject:\n",
    "            relations.append(subject[0])\n",
    "            \n",
    "        modifier = list(filter(lambda x : 'Modifier' in x[1] and x[2] == 'Action:'+t[0], rels))\n",
    "        if modifier:\n",
    "            relations.append(modifier[0])\n",
    "            # it never happens...\n",
    "\n",
    "    if not relations:\n",
    "        print(\"Token {} has no relation.\".format(t))\n",
    "        global relation_missing\n",
    "        relation_missing += 1\n",
    "    \n",
    "    words = [t[3]]\n",
    "    \n",
    "    for r in relations:\n",
    "        \n",
    "        if include_subject and 'Subject' in r[1]: \n",
    "            # subject -> action\n",
    "            token_id = r[1].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "                        \n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                words.insert(0, token_word[0][3])\n",
    "            \n",
    "        else:\n",
    "            # action -> object, action -> mod -> object\n",
    "            token_id = r[2].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                if token_word[0][3][0].isupper():\n",
    "                    words.insert(0, token_word[0][3])\n",
    "                else:\n",
    "                    words.append(token_word[0][3])\n",
    "    \n",
    "    # relation gathered and here we create the dataset\n",
    "    # id prefer it in anotehr function but whatever\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    annotation_result = process_annotation(annotations, words, t)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1\n",
    "        print(words)\n",
    "    \n",
    "    return words, annotation_result             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_coords(t, text, off=0):\n",
    "    indices = t[2]\n",
    "    indices = indices.split(';') if ';' in indices else [indices]\n",
    "    real_token = ''\n",
    "    indices = [[int(idx) for idx in i.split()] for i in indices]\n",
    "    for i in indices:\n",
    "        real_token += text[i[0]+off : i[1]+off] + ' '\n",
    "        \n",
    "    while '- ' in real_token:\n",
    "        real_token = real_token.replace('- ', '')\n",
    "    return real_token.strip(), (indices[0][0]+off, indices[-1][1]+off)\n",
    "\n",
    "def get_token_neighbors(token, coords, text, neighbors):\n",
    "    \n",
    "    left = (neighbors-1)/2\n",
    "    right = left\n",
    "    \n",
    "    l_idx = coords[0] - 1\n",
    "    r_idx = coords[1] + 1\n",
    "    \n",
    "    words = []\n",
    "    while left:\n",
    "        while text[l_idx].isspace():\n",
    "            l_idx -= 1\n",
    "        word = ''\n",
    "        while not text[l_idx].isspace():\n",
    "            word += text[l_idx]\n",
    "            l_idx -= 1\n",
    "        \n",
    "        word = word[::-1]\n",
    "        if word[0] == '<' or word[-1] == '>': #html tag\n",
    "            l_idx -=1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.insert(0, word)\n",
    "            left -= 1\n",
    "    \n",
    "    words.append(token)\n",
    "    while right:\n",
    "        while text[r_idx].isspace():\n",
    "            r_idx += 1\n",
    "        word = ''\n",
    "        while not text[r_idx].isspace():\n",
    "            word += text[r_idx]\n",
    "            r_idx += 1\n",
    "            \n",
    "        if word[0] == '<' or word[-1] == '>':\n",
    "            r_idx += 1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.append(word)\n",
    "            right -= 1\n",
    "            \n",
    "    return words\n",
    "        \n",
    "def gather_data(words, anns, t):\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    annotation_result = process_annotation(annotations, words, t)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1\n",
    "        \n",
    "    return annotation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> butterfly-corporate-spies-out-for-financial-gain.ann\n",
      "\n",
      ">>> Equation_group_questions_and_answers.ann\n",
      "\n",
      ">>> Dissecting-LinuxMoose.ann\n",
      "Token ('T136', 'Action', '52031 52036', 'reach') has no annotation.\n",
      "Token ('T136', 'Action', '52031 52036', 'reach') has no annotation.\n",
      "['reach', 'the configuration C&C server']\n",
      "Token ('T136', 'Action', '52031 52036', 'reach') has no annotation.\n",
      "['it', 'reach', 'the configuration C&C server']\n",
      "\n",
      ">>> Dissecting-the-Kraken.ann\n",
      "Token ('T16', 'Action', '10699 10705', 'update') has no relation.\n",
      "Token ('T16', 'Action', '10699 10705', 'update') has no relation.\n",
      "Token ('T17', 'Action', '10710 10719', 'uninstall') has no relation.\n",
      "Token ('T17', 'Action', '10710 10719', 'uninstall') has no relation.\n",
      "\n",
      ">>> DEEP_PANDA_Sakula.ann\n"
     ]
    }
   ],
   "source": [
    "set_type = 'Test'\n",
    "path = root + sets[set_type] + ann\n",
    "data, undefined = init_data()\n",
    "\n",
    "relation_mode = True\n",
    "include_subject = True\n",
    "\n",
    "if relation_mode:\n",
    "    if include_subject:\n",
    "        label = 'text-rel-subj'\n",
    "    else:\n",
    "        label = 'text-rel'\n",
    "else:\n",
    "    label = 'text-neigh'\n",
    "    \n",
    "relation_missing = 0\n",
    "annotation_missing = 0\n",
    "tokens_count = 0\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "\n",
    "    if filename.endswith('.ann'):\n",
    "        print(\"\\n>>> {}\".format(filename))\n",
    "\n",
    "        with open(path + '/' + filename) as ann_file:\n",
    "            content = ann_file.read()\n",
    "\n",
    "            # match lines with a token            \n",
    "            tokens = re.findall(token_pattern, content)\n",
    "            relations = re.findall(rel_pattern, content)\n",
    "            annotations = re.findall(ann_pattern, content)\n",
    "\n",
    "            with open(path + \"/\" + filename.replace(\".ann\", \".txt\")) as txt_file:\n",
    "                text = txt_file.read()\n",
    "                text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "                offset = 0\n",
    "                for t in tokens:\n",
    "                    if t[1] == 'Action':\n",
    "\n",
    "                        ann_token = t[3]\n",
    "                        tokens_count += 1\n",
    "                        \n",
    "                        # WHICH TOKEN?\n",
    "                        data['token'].append(ann_token)\n",
    "\n",
    "                        idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "\n",
    "                        if ann_token != idx_token:\n",
    "                            # this fixes errors occuring in one file - indices are invalid\n",
    "                            where_is = text.find(ann_token)\n",
    "                            offset = where_is - int(t[2].split()[0])\n",
    "                            idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "\n",
    "                        # NEIGHBOURS\n",
    "                        words = get_token_neighbors(ann_token, coords, text, 9)\n",
    "                        annotation_result = gather_data(words, annotations, t)\n",
    "                        if not words or not annotation_result:\n",
    "                            print(\"NEIGHBOURS error\")\n",
    "                        else:\n",
    "                            data['text-neigh'].append(words)\n",
    "\n",
    "                        # BASIC RELATIONS\n",
    "                        words, annotation_result = get_token_words_relation(t, tokens, \n",
    "                                                                        relations, annotations,\n",
    "                                                                       include_subject=False)\n",
    "                        if not words or not annotation_result:\n",
    "                            print(\"RELATIONS error\")\n",
    "                        else:\n",
    "                            data['text-rel'].append(words)\n",
    "\n",
    "                        # BASIC RELATIONS + SUBJECT->ACTION RELATION\n",
    "                        words, annotation_result = get_token_words_relation(t, tokens, \n",
    "                                                                        relations, annotations,\n",
    "                                                                       include_subject=True)\n",
    "                        if not words or not annotation_result:\n",
    "                            print(\"RELATIONS+SUBJ error\")\n",
    "                        else:\n",
    "                            data['text-rel-subj'].append(words)\n",
    "                        \n",
    "\n",
    "                        # WHICH CLASSES?\n",
    "                        data['ActionName'].append(annotation_result[0])\n",
    "                        data['Capability'].append(annotation_result[1])\n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionName 110\n",
      "Capability 110\n",
      "token 110\n",
      "text-rel-subj 110\n",
      "text-rel 110\n",
      "text-neigh 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActionName</th>\n",
       "      <th>Capability</th>\n",
       "      <th>token</th>\n",
       "      <th>text-rel-subj</th>\n",
       "      <th>text-rel</th>\n",
       "      <th>text-neigh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>Other</td>\n",
       "      <td>obtaining</td>\n",
       "      <td>The attackers obtaining access to specific sys...</td>\n",
       "      <td>obtaining access to specific systems of intere...</td>\n",
       "      <td>The attackers focused on obtaining access to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>Other</td>\n",
       "      <td>eavesdropped</td>\n",
       "      <td>they eavesdropped on email conversations</td>\n",
       "      <td>eavesdropped on email conversations</td>\n",
       "      <td>access, they presumably then eavesdropped on e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>Other</td>\n",
       "      <td>insert</td>\n",
       "      <td>they insert fraudulent emails</td>\n",
       "      <td>insert fraudulent emails</td>\n",
       "      <td>a position to potentially insert fraudulent em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>command_and_control</td>\n",
       "      <td>used</td>\n",
       "      <td>used a command-and-control (C&amp;C) server in an ...</td>\n",
       "      <td>used a command-and-control (C&amp;C) server in an ...</td>\n",
       "      <td>a command-and-control (C&amp;C) server used in an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>File</td>\n",
       "      <td>-</td>\n",
       "      <td>executed</td>\n",
       "      <td>Bda9.tmp executed</td>\n",
       "      <td>Bda9.tmp executed</td>\n",
       "      <td>Explorer. Bda9.tmp was then executed and went ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ActionName           Capability         token  \\\n",
       "0          -                Other     obtaining   \n",
       "1          -                Other  eavesdropped   \n",
       "2          -                Other        insert   \n",
       "3          -  command_and_control          used   \n",
       "4       File                    -      executed   \n",
       "\n",
       "                                       text-rel-subj  \\\n",
       "0  The attackers obtaining access to specific sys...   \n",
       "1           they eavesdropped on email conversations   \n",
       "2                      they insert fraudulent emails   \n",
       "3  used a command-and-control (C&C) server in an ...   \n",
       "4                                  Bda9.tmp executed   \n",
       "\n",
       "                                            text-rel  \\\n",
       "0  obtaining access to specific systems of intere...   \n",
       "1                eavesdropped on email conversations   \n",
       "2                           insert fraudulent emails   \n",
       "3  used a command-and-control (C&C) server in an ...   \n",
       "4                                  Bda9.tmp executed   \n",
       "\n",
       "                                          text-neigh  \n",
       "0  The attackers focused on obtaining access to s...  \n",
       "1  access, they presumably then eavesdropped on e...  \n",
       "2  a position to potentially insert fraudulent em...  \n",
       "3  a command-and-control (C&C) server used in an ...  \n",
       "4  Explorer. Bda9.tmp was then executed and went ...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "[print(key, len(data[key])) for key in data.keys()]\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "    key : data[key] for key in data.keys()\n",
    "})\n",
    "\n",
    "# dataset['text-rel-lengths'] = dataset['text-rel'].apply(lambda x : len((' '.join(x)).split()))\n",
    "# dataset['text-rel-subj-lengths'] = dataset['text-rel-subj'].apply(lambda x : len((' '.join(x)).split()))\n",
    "\n",
    "# print(np.mean(dataset['text-rel-lengths']))\n",
    "# print(np.median(dataset['text-rel-lengths']))\n",
    "\n",
    "# print(np.mean(dataset['text-rel-subj-lengths']))\n",
    "# print(np.median(dataset['text-rel-subj-lengths']))\n",
    "\n",
    "for column in ['text-rel', 'text-neigh', 'text-rel-subj']:\n",
    "    dataset[column] = dataset[column].apply(' '.join)\n",
    "    dataset[column] = dataset[column].apply(lambda x : re.sub(r\"[“”\\[\\]]\", \"\", x))  \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Test__________ \n",
      "\n",
      "-          76\n",
      "File       15\n",
      "Other      12\n",
      "Network     7\n",
      "Name: ActionName, dtype: int64 \n",
      "\n",
      "Other                    63\n",
      "-                        22\n",
      "command_and_control      15\n",
      "infection_propagation    10\n",
      "Name: Capability, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"_\" * 10 + set_type + \"_\" * 10, '\\n')\n",
    "dataset.to_csv('All-{}.csv'.format(set_type))\n",
    "print(dataset.ActionName.value_counts(), \"\\n\")\n",
    "print(dataset.Capability.value_counts(), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
