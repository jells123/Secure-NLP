{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing SMOTE based on\n",
    "https://www.kaggle.com/qianchao/smote-with-imbalance-data/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded data!\n",
      "\n",
      "['label', 'text-rel', 'label_num', 'tokens', 'text-neigh', 'token', 'text-rel-processed', 'text-neigh-processed', 'text-rel-tokens', 'text-neigh-tokens']\n"
     ]
    }
   ],
   "source": [
    "import data_helpers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_main = True\n",
    "\n",
    "main_cats = ['ActionName', 'Capability']\n",
    "dataframes = {cat : {} for cat in main_cats} if load_main else {}\n",
    "\n",
    "for cat in main_cats:\n",
    "    path = os.getcwd() + \"/Dataframes/\" + cat + \"/Processed/\"\n",
    "    ls = os.listdir(path)\n",
    "    for file in ls:\n",
    "        if (load_main and cat in file) or (not load_main and cat not in file):\n",
    "            df = pd.read_csv(path + file, encoding='utf-8')\n",
    "\n",
    "            df.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "            df.drop([\"a\"], axis=1, inplace=True)\n",
    "\n",
    "            df.rename({\"Unnamed: 0.1\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "            df.drop([\"a\"], axis=1, inplace=True)\n",
    "\n",
    "            df_type = file.split('-')[1]\n",
    "            if load_main:\n",
    "                dataframes[cat][df_type] = df\n",
    "            else:\n",
    "                sub_cat = file.split('-')[0]\n",
    "                if sub_cat not in dataframes.keys():\n",
    "                    dataframes[sub_cat] = {}\n",
    "                dataframes[sub_cat][df_type] = df\n",
    "\n",
    "if not load_main:\n",
    "    main_cats = list(dataframes.keys())\n",
    "\n",
    "print(\"\\tLoaded data!\\n\")\n",
    "if load_main:\n",
    "    print(list(dataframes['ActionName']['Train'].columns))\n",
    "else:\n",
    "    print(list(dataframes['File']['Train'].columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Loaded embeddings model!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(path):\n",
    "    mapping = dict()\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            splitted = line.split(\" \")\n",
    "            if len(splitted) <= 2:\n",
    "                continue\n",
    "            mapping[splitted[0]] = np.array(splitted[1:], dtype=float) # stwórz słownik słowo -> wektor \n",
    "    return mapping\n",
    "\n",
    "mapping = load_embeddings('Embedding-Models/super-model.txt') \n",
    "print(\"\\t Loaded embeddings model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate simple average embeddings for given sentences,\n",
    "based on file loaded (for example vectors of length 50).\n",
    "\n",
    "Skips words, that don't exist in mapping!\n",
    "Therefore some possibly meaningful words (in security context) are skipped.\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from data_helpers import clean_sentence\n",
    "\n",
    "import re\n",
    "word_pattern = re.compile(r'[^a-zA-Z0-9-]')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def documents_to_ave_embeddings(docs, embeddings):\n",
    "    result = []\n",
    "    for idx, doc in enumerate(docs):\n",
    "        vectors = []\n",
    "        \n",
    "        new_doc = clean_sentence(doc)\n",
    "        new_doc = re.sub(word_pattern, \" \", new_doc)\n",
    "        \n",
    "        words = word_tokenize(new_doc)\n",
    "        words = [word.lower() for word in words if len(word) > 1 and len(word) <= 25]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in embeddings.keys():\n",
    "                vectors.append(embeddings[word])\n",
    "          \n",
    "        if vectors:\n",
    "            result.append(np.average(vectors, axis=0))\n",
    "        else:\n",
    "            result.append(np.zeros(shape=result[0].shape))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Capability\n",
      "\n",
      "Before OverSampling, counts of label '1': 2817\n",
      "Before OverSampling, counts of label '0': 531\n",
      "After OverSampling, counts of label '1': 2817\n",
      "After OverSampling, counts of label '0': 2817\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tokens_count = 0\n",
    "missing_tokens = 0\n",
    "\n",
    "cat = 'Capability'\n",
    "column = 'text-rel'\n",
    "\n",
    "print(\">>> {}\\n\".format(cat))\n",
    "\n",
    "train = dataframes[cat]['Train']\n",
    "test = dataframes[cat]['Test']\n",
    "\n",
    "y_train, y_test = train['label_num'], test['label_num']\n",
    "\n",
    "train[column] = train[column].apply(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "test[column] = test[column].apply(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "train_transformed = documents_to_ave_embeddings(train[column], mapping)\n",
    "test_transformed = documents_to_ave_embeddings(test[column], mapping)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {}\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "train_res, y_train_res = sm.fit_sample(train_transformed, y_train.ravel())\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=2.0 ...........................................................\n",
      "[CV] C=2.0 ...........................................................\n",
      "[CV] .................. C=2.0, score=0.7928531546621999, total=   1.0s\n",
      "[CV] C=2.0 ...........................................................\n",
      "[CV] .................. C=2.0, score=0.8434285714285713, total=   1.2s\n",
      "[CV] C=4.0 ...........................................................\n",
      "[CV] .................. C=2.0, score=0.8225712623618383, total=   1.0s\n",
      "[CV] C=4.0 ...........................................................\n",
      "[CV] ................... C=4.0, score=0.797532249018508, total=   1.3s\n",
      "[CV] C=4.0 ...........................................................\n",
      "[CV] .................... C=4.0, score=0.84505431675243, total=   1.5s\n",
      "[CV] C=6.0 ...........................................................\n",
      "[CV] .................. C=4.0, score=0.8288707799767172, total=   1.4s\n",
      "[CV] C=6.0 ...........................................................\n",
      "[CV] .................... C=6.0, score=0.79842784952274, total=   1.7s\n",
      "[CV] C=6.0 ...........................................................\n",
      "[CV] .................... C=6.0, score=0.84505431675243, total=   1.5s\n",
      "[CV] C=8.0 ...........................................................\n",
      "[CV] .................. C=8.0, score=0.7968486212718063, total=   1.6s\n",
      "[CV] C=8.0 ...........................................................\n",
      "[CV] .................. C=6.0, score=0.8316831683168316, total=   2.6s\n",
      "[CV] C=8.0 ...........................................................\n",
      "[CV] .................. C=8.0, score=0.8478260869565217, total=   1.6s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] .................. C=8.0, score=0.8344988344988344, total=   2.4s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ................. C=10.0, score=0.7959527824620574, total=   1.7s\n",
      "[CV] C=10.0 ..........................................................\n",
      "[CV] ................. C=10.0, score=0.8466819221967964, total=   2.0s\n",
      "[CV] C=12.0 ..........................................................\n",
      "[CV] .................. C=10.0, score=0.836937463471654, total=   2.6s\n",
      "[CV] C=12.0 ..........................................................\n",
      "[CV] ................. C=12.0, score=0.7975253093363329, total=   2.1s\n",
      "[CV] C=12.0 ..........................................................\n",
      "[CV] ................. C=12.0, score=0.8473413379073756, total=   2.1s\n",
      "[CV] C=14.0 ..........................................................\n",
      "[CV] ................. C=12.0, score=0.8376168224299064, total=   2.5s\n",
      "[CV] C=14.0 ..........................................................\n",
      "[CV] .................. C=14.0, score=0.796400449943757, total=   2.0s\n",
      "[CV] C=14.0 ..........................................................\n",
      "[CV] ................. C=14.0, score=0.8382953882078226, total=   1.8s\n",
      "[CV] C=16.0 ..........................................................\n",
      "[CV] ................. C=14.0, score=0.8478260869565217, total=   2.2s\n",
      "[CV] C=16.0 ..........................................................\n",
      "[CV] ................. C=16.0, score=0.7966197183098592, total=   2.1s\n",
      "[CV] C=16.0 ..........................................................\n",
      "[CV] ................. C=16.0, score=0.8469914040114613, total=   2.1s\n",
      "[CV] C=18.0 ..........................................................\n",
      "[CV] ................. C=16.0, score=0.8398369248689574, total=   2.3s\n",
      "[CV] C=18.0 ..........................................................\n",
      "[CV] ................. C=18.0, score=0.7959413754227734, total=   2.2s\n",
      "[CV] C=18.0 ..........................................................\n",
      "[CV] ................. C=18.0, score=0.8463302752293578, total=   2.1s\n",
      "[CV] C=20.0 ..........................................................\n",
      "[CV] ................. C=18.0, score=0.8373177842565599, total=   2.3s\n",
      "[CV] C=20.0 ..........................................................\n",
      "[CV] ................... C=20.0, score=0.79639029892837, total=   2.5s\n",
      "[CV] C=20.0 ..........................................................\n",
      "[CV] ................. C=20.0, score=0.8466398621481906, total=   2.2s\n",
      "[CV] ................. C=20.0, score=0.8384839650145772, total=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  30 out of  30 | elapsed:   29.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'C': array([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Grid search because it's fancy.\n",
    "'''\n",
    "\n",
    "parameters = {\n",
    "    'C' : np.linspace(2, 20, 10)\n",
    "}\n",
    "clf = GridSearchCV(\n",
    "    LogisticRegression(), \n",
    "    parameters, \n",
    "#     cv=5, # means 5-fold cross validation\n",
    "    scoring='f1',\n",
    "    n_jobs=2,\n",
    "    verbose=3)\n",
    "clf.fit(train_res, y_train_res.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 16.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBefore OverSampling:\n",
      "Train F1: 0.9488823427482238\n",
      "Test F1: 0.8972431077694237\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.36      0.45        47\n",
      "          1       0.86      0.94      0.90       190\n",
      "\n",
      "avg / total       0.81      0.83      0.81       237\n",
      "\n",
      "\tAfter OverSampling:\n",
      "Train F1: 0.9024168635289841\n",
      "Test F1: 0.8670212765957447\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.51      0.49        47\n",
      "          1       0.88      0.86      0.87       190\n",
      "\n",
      "avg / total       0.80      0.79      0.79       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    C = 16.0\n",
    "#     class_weight='balanced'\n",
    ")\n",
    "clf.fit(train_transformed, train['label_num']) # zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "\n",
    "train_f1 = f1_score(clf.predict(train_transformed), train['label_num'])\n",
    "\n",
    "y_pred = clf.predict(test_transformed)\n",
    "test_f1 = f1_score(y_pred, test['label_num'])\n",
    "\n",
    "print(\"\\tBefore OverSampling:\")\n",
    "print(\"Train F1: {}\".format(train_f1))\n",
    "print(\"Test F1: {}\".format(test_f1))\n",
    "\n",
    "print(classification_report(test['label_num'], y_pred))\n",
    "\n",
    "clf.fit(train_res, y_train_res.ravel())\n",
    "\n",
    "train_f1 = f1_score(clf.predict(train_res), y_train_res.ravel())\n",
    "\n",
    "y_pred = clf.predict(test_transformed)\n",
    "test_f1 = f1_score(y_pred, test['label_num'])\n",
    "\n",
    "print(\"\\tAfter OverSampling:\")\n",
    "print(\"Train F1: {}\".format(train_f1))\n",
    "print(\"Test F1: {}\".format(test_f1))\n",
    "\n",
    "print(classification_report(test['label_num'], y_pred))\n",
    "# c_m = confusion_matrix(test['label_num'], y_pred)\n",
    "# tn, fp, fn, tp = c_m.ravel()\n",
    "# # print(\"TN: {}, FP: {}, FN: {}, TP: {}\".format(tn, fp, fn, tp))\n",
    "# print(c_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, OverSampling did help to recognize negative class for Capability. \n",
    "However - it is not necessarily what we expect, because f1-score for positive class decreased, \n",
    "so more positive classes will be omitted - which leads to no Capability-category for a given sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
