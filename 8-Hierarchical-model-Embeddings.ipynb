{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class HierarchicalModel:\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "        self.train_results = None\n",
    "        self.test_results = None\n",
    "        \n",
    "        self.main_class_predictor = lambda x: 1 if x[1] > x[0] else 1\n",
    "        self.sub_class_predictor = None\n",
    "    \n",
    "    def loadData(self, path, main_cats_list):\n",
    "        dataframes = {}\n",
    "\n",
    "        print(\"Loading data...\\n\")\n",
    "        ls = os.listdir(path)\n",
    "\n",
    "        for file in ls:\n",
    "            if '.csv' in file and 'old' not in file:\n",
    "\n",
    "                print(path + file)\n",
    "                df = pd.read_csv(path + file, encoding='utf-8')\n",
    "\n",
    "                for cname in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"label\"]:\n",
    "                    if cname in df.columns:\n",
    "                        df.rename({cname:\"a\"}, axis=\"columns\", inplace=True)\n",
    "                        df.drop([\"a\"], axis=1, inplace=True)\n",
    "                df_type = file.split('-')[1]\n",
    "\n",
    "                dataframes[df_type] = df\n",
    "\n",
    "        self.train = dataframes['Train']\n",
    "        self.test = dataframes['Test']\n",
    "        \n",
    "        categories, mapper = {}, {}\n",
    "        for mc in main_cats_list:\n",
    "            categories[mc] = list(sorted(set(filter(lambda x : x != '-', self.train[mc]))))\n",
    "            mapper[mc] = {}\n",
    "            for cat in np.unique(self.train[mc]):\n",
    "                mapper[mc][cat] = np.unique(self.train[self.train[mc] == cat][\"{}_num\".format(mc)])[0]\n",
    "        \n",
    "        self.mapper = mapper\n",
    "        self.categories = categories\n",
    "        self.main_cats = main_cats_list\n",
    "        \n",
    "        print(\"\\nLoaded categories:\\n\")\n",
    "        print(categories)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def predictMainClass(self, data_c, pipeline, report=False, undefined=\"-\"):\n",
    "        \n",
    "        if not self.train_results:\n",
    "            self.train_results = pd.DataFrame()\n",
    "        if not self.test_results:\n",
    "            self.test_results = pd.DataFrame()\n",
    "        \n",
    "        if report:\n",
    "            print(\"\\n > > > MAIN CLASS CLASSIFICATION > > >\\n\")\n",
    "        else:\n",
    "            print(\"\\nClassifying main classes...\")\n",
    "            \n",
    "        for main_cat in self.main_cats:\n",
    "            \n",
    "            if report:\n",
    "                print(\"-\" * 52)\n",
    "\n",
    "            train_labels = self.train[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "            test_labels = self.test[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "\n",
    "            # train pipeline\n",
    "            pipeline.fit(self.train[data_c], train_labels)\n",
    "\n",
    "            # store train set results\n",
    "            self.train_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "            self.train_results['{}-def_true'.format(main_cat)] = train_labels\n",
    "\n",
    "            train_pred = pipeline.predict(self.train[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TRAIN results:\".format(main_cat))\n",
    "                print(classification_report(train_labels, train_pred))\n",
    "\n",
    "            # store test set results\n",
    "            self.test_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "            self.test_results['{}-def_true'.format(main_cat)] = test_labels\n",
    "\n",
    "            test_pred = pipeline.predict(self.test[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TEST results:\".format(main_cat))\n",
    "                print(classification_report(test_labels, test_pred))\n",
    "\n",
    "            # prepare columns to store our final predictions\n",
    "            self.train_results['{}_PREDICTION'.format(main_cat)] = '?'    \n",
    "            self.test_results['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            # additional column for test-set\n",
    "            # replace '?' with '-' according to pipeline's prediction\n",
    "            # - if it's very certain about answering NO, it won't be taken into consideration\n",
    "            # in next level of classification\n",
    "\n",
    "            self.test['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            for i in range(self.test_results.shape[0]):\n",
    "                row = self.test_results.loc[i]\n",
    "                def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                pred = self.main_class_predictor(def_pred)\n",
    "                if pred == 0:\n",
    "                    self.test.at[i, \"{}_PREDICTION\".format(main_cat)] = undefined\n",
    "                    \n",
    "        if not report:\n",
    "            print(\"Done!\")\n",
    "                    \n",
    "    def predictSubClass(self, data_c, pipeline, undefined=\"-\"):\n",
    "\n",
    "        print(\"\\nClassifying subclasses [binary classifiers!]...\")\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            print('\\n' + main_cat + \":\")\n",
    "            cats = self.categories[main_cat]\n",
    "\n",
    "            for cat in cats:\n",
    "                print(\"- \" + cat)\n",
    "\n",
    "                # subset of train set where main_cat is defined\n",
    "                train_subset = self.train.loc[self.train[main_cat] != undefined]\n",
    "                # subset of test set where main_cat is defined, according to previous classification stage\n",
    "                test_subset = self.test.loc[self.test[\"{}_PREDICTION\".format(main_cat)] != undefined] # !!!\n",
    "\n",
    "                # prepare binary labels: 1 for this class, 0 for any other\n",
    "                train_labels = train_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "                test_labels = test_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "\n",
    "                # train pipeline\n",
    "                pipeline.fit(train_subset[data_c], train_labels)\n",
    "\n",
    "                pred_label = '{}_prediction'.format(cat)\n",
    "                true_label = '{}_true'.format(cat)\n",
    "\n",
    "                # store results \n",
    "                self.train_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "                self.train_results[true_label] = train_labels\n",
    "\n",
    "                self.test_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "                self.test_results[true_label] = test_labels\n",
    "\n",
    "                # fix NaN issues...\n",
    "                for df in [self.train_results, self.test_results]:\n",
    "                    df[true_label] = df[true_label].map(lambda x : '-' if math.isnan(x) else int(x))\n",
    "          \n",
    "        print(\"\\nMaking decision...\")\n",
    "        \n",
    "        for df in [self.train_results, self.test_results]:\n",
    "            # iterate over rows\n",
    "            for i in range(df.shape[0]):\n",
    "                row = df.loc[i]\n",
    "\n",
    "                for main_cat in self.main_cats:\n",
    "                    # get main class prediction\n",
    "                    def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                    # make decision :)\n",
    "                    pred = self.main_class_predictor(def_pred)\n",
    "                    # get truth (mostly for debugging)\n",
    "                    truth = row['{}-def_true'.format(main_cat)]\n",
    "\n",
    "                    if pred:\n",
    "                        # if predicted as defined, get sub classes\n",
    "                        cats = self.categories[main_cat]\n",
    "\n",
    "                        positive_scores = []\n",
    "                        negative_scores = []\n",
    "\n",
    "                        # gather negative and positive votes\n",
    "                        for cat in cats:\n",
    "                            cat_prob = row['{}_prediction'.format(cat)]\n",
    "                            negative_scores.append(cat_prob[0] * def_pred[0])\n",
    "                            positive_scores.append(cat_prob[1] * def_pred[1])\n",
    "\n",
    "                        # get best scores for negative and positive answer\n",
    "                        pos_idx = np.argmax(positive_scores)\n",
    "                        neg_idx = np.argmax(negative_scores)\n",
    "\n",
    "                        if positive_scores[pos_idx] > negative_scores[neg_idx]:\n",
    "                            best_class = cats[pos_idx]   \n",
    "                        else:\n",
    "                            best_class = '-'\n",
    "\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = best_class\n",
    "                    else:\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = \"-\"\n",
    "                        \n",
    "        print(\"Done!\")\n",
    "      \n",
    "    def transformDocsToEmbeddings(self, docs, embeddings, slice_size=None, bin_features=False):\n",
    "        from nltk import word_tokenize\n",
    "        from data_helpers import clean_sentence\n",
    "        \n",
    "        import re\n",
    "        word_pattern = re.compile(r'[^a-zA-Z0-9-]')\n",
    "\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        result = []\n",
    "        for idx, doc in enumerate(docs):\n",
    "            vectors = []\n",
    "\n",
    "            if bin_features:\n",
    "                new_doc, features = clean_sentence(doc, get_features=True)   \n",
    "                f_to_bin = np.array(list(map(int, features.values())), dtype=np.float64)\n",
    "            else:\n",
    "                new_doc = clean_sentence(doc, get_features=False)   \n",
    "\n",
    "            new_doc = re.sub(word_pattern, \" \", new_doc)\n",
    "\n",
    "            words = word_tokenize(new_doc)\n",
    "            words = [word.lower() for word in words if len(word) > 1 and len(word) <= 25]\n",
    "\n",
    "            for word in words:\n",
    "\n",
    "                if word in embeddings.keys():\n",
    "                    vec = embeddings[word]\n",
    "\n",
    "                    if slice_size is not None:\n",
    "                        first_slice = vec[:slice_size]\n",
    "                        second_slice = vec[vec_size : vec_size+slice_size]\n",
    "                        vec = np.concatenate((first_slice, second_slice))\n",
    "\n",
    "                    vectors.append(vec)\n",
    "                else:\n",
    "                    pass\n",
    "#                     global missing_tokens\n",
    "#                     missing_tokens += 1\n",
    "\n",
    "            if vectors:\n",
    "                avg_vec = np.average(vectors, axis=0)\n",
    "            else:\n",
    "                avg_vec = np.zeros(shape=result[0].shape)\n",
    "\n",
    "            if bin_features:\n",
    "                avg_vec = np.concatenate((avg_vec, f_to_bin))\n",
    "\n",
    "            result.append(avg_vec)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def addEmbeddings(self, in_column, out_column, mapping, ss=100, get_binary_features=False):\n",
    "        \n",
    "        train_clean = self.train[in_column].apply(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "        test_clean = self.test[in_column].apply(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "        train_transformed = self.transformDocsToEmbeddings(train_clean, mapping, slice_size=ss, bin_features=get_binary_features)\n",
    "        test_transformed = self.transformDocsToEmbeddings(test_clean, mapping, slice_size=ss, bin_features=get_binary_features)\n",
    "    \n",
    "        self.train[out_column] = list(train_transformed)\n",
    "        self.test[out_column] = list(test_transformed)\n",
    "    \n",
    "    def showResults(self):\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            self.train[\"{}-PREDICTION_num\".format(main_cat)] = self.train_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "            self.test[\"{}-PREDICTION_num\".format(main_cat)] = self.test_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "\n",
    "        for main_cat in self.main_cats:\n",
    "            print()\n",
    "            print(\"-\"*20 + \" \" + main_cat + \" \" + \"-\"*20)\n",
    "            print(self.mapper[main_cat])\n",
    "            \n",
    "            print(\"\\n>>> TRAIN:\")\n",
    "            print(classification_report(self.train['{}_num'.format(main_cat)], self.train['{}-PREDICTION_num'.format(main_cat)]),\n",
    "#                  target_names=self.mapper[main_cat]\n",
    "                 )\n",
    "            \n",
    "            print(\"\\n>>> TEST:\")\n",
    "            print(classification_report(self.test['{}_num'.format(main_cat)], self.test['{}-PREDICTION_num'.format(main_cat)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    mapping = dict()\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            splitted = line.split(\" \")\n",
    "            if len(splitted) <= 2:\n",
    "                continue\n",
    "            mapping[splitted[0]] = np.array(splitted[1:], dtype=float) # stwórz słownik słowo -> wektor \n",
    "    return mapping\n",
    "\n",
    "vec_size = 300\n",
    "\n",
    "# glove_mapping = load_embeddings('glove/glove.6B.{}d.txt'.format(vec_size)) \n",
    "# my_mapping = load_embeddings('Embedding-Models/size{}-window10.txt'.format(vec_size)) \n",
    "super_mapping = load_embeddings('Embedding-Models/super-model.txt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare simple pipeline\n",
    "'''\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([          \n",
    "#     ('vectorizer',\n",
    "#      CountVectorizer(\n",
    "#          preprocessor=dummy, \n",
    "#          ngram_range=(1, 4),\n",
    "#          analyzer='word',\n",
    "#          binary=False\n",
    "#      )),\n",
    "    ('clf', \n",
    "     LogisticRegression(\n",
    "         class_weight='balanced',\n",
    "         solver='liblinear'\n",
    "     )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Train-P.csv\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Test-P.csv\n",
      "\n",
      "Loaded categories:\n",
      "\n",
      "{'ActionName': ['File', 'Network', 'Other'], 'Capability': ['command_and_control', 'infection_propagation', 'other']}\n",
      "\n",
      "Classifying main classes...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-33c2cdab5a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# trzeba przerobić na LISTę\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictMainClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-embedding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# hm.predictSubClass('text-embedding', pipeline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# hm.showResults()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-932153f71029>\u001b[0m in \u001b[0;36mpredictMainClass\u001b[0;34m(self, data_c, pipeline, report, undefined)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# train pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# store train set results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1285\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "hm = HierarchicalModel()\n",
    "hm.loadData(os.getcwd() + \"/Dataframes/All/Processed/\", [\"ActionName\", \"Capability\"])\n",
    "hm.main_class_predictor = lambda predictions: 0 if predictions[0] - predictions[1] > 0.5 else 1\n",
    "\n",
    "hm.addEmbeddings(\"text-rel\", \"text-embedding\", super_mapping)\n",
    "\n",
    "# pipeline.fit(list(hm.train['text-embedding']), hm.train[\"Capability\"].map(lambda x : 0 if x == '-' else 1))\n",
    "# trzeba przerobić na LISTę\n",
    "\n",
    "hm.predictMainClass('text-embedding', pipeline, report=False)\n",
    "# hm.predictSubClass('text-embedding', pipeline)\n",
    "# hm.showResults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
