{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis file gathers all previously selected categories:\\n1. ActionName\\n    File\\n    Network\\n    Other\\n2. Capability\\n    infection_propagation\\n    ...something\\n    other\\ninto one DataFrame, instead of gathering binary data into files \\n(for example ActionName - NoActionName)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This file gathers all previously selected categories:\n",
    "1. ActionName\n",
    "    File\n",
    "    Network\n",
    "    Other\n",
    "2. Capability\n",
    "    infection_propagation\n",
    "    ...something\n",
    "    other\n",
    "into one DataFrame, instead of gathering binary data into files \n",
    "(for example ActionName - NoActionName)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jells123/Documents/ENGINEER/DATA/1-Raw data/data/'\n",
    "ann = '/annotations'\n",
    "\n",
    "sets = {\n",
    "    'Train' : 'train',\n",
    "    'Dev' : 'dev',\n",
    "    'Test' : 'test_3'\n",
    "}\n",
    "\n",
    "rel_pattern = r'(?P<r_id>R[0-9]+)\\s+[A-Za-z]+\\s+(?P<r_first>[A-Za-z]+:T[0-9]+)\\s+(?P<r_second>[A-Za-z]+:T[0-9]+)'\n",
    "ann_pattern = r'(?P<a_id>A[0-9]+)\\s+(?P<cat>[A-Za-z]+)\\s+(?P<t_id>T[0-9]+)\\s+(?P<text>[^\\n]+)'\n",
    "token_pattern = r'(?P<t_id>T[0-9]+)\\s+(?P<t_type>[A-Z][a-z]+) (?P<indices>[0-9]+ [0-9]+[^\\t]+)\\t(?P<t_text>.+)\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data():\n",
    "    data = {\n",
    "        'ActionName' : [],\n",
    "        'Capability' : [],\n",
    "        \n",
    "        'token' : [],\n",
    "        'text-rel-subj' : [],\n",
    "        'text-rel' : [],\n",
    "        'text-neigh' : []\n",
    "    }\n",
    "    undefined = None\n",
    "    return data, undefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation(anns, words, token):\n",
    "    \n",
    "#     data['Text'].append(words)\n",
    "    \n",
    "    current_annotations = {}\n",
    "    for ann in anns:\n",
    "        m = re.search(r'(?P<id>[0-9]{3}):(?P<type>[A-Za-z]+)-(?P<name>.+)\\s*', ann[3])\n",
    "        typ, name = m.group('type'), m.group('name')\n",
    "        typ, name = re.sub(r'\\/', '_', typ), re.sub(r'\\/', '_', name)\n",
    "        main_cat = ann[1]\n",
    "        if main_cat == 'Capability':\n",
    "            # special treatment for Capability because there are no sub-categories\n",
    "            current_annotations[main_cat] = name\n",
    "        else:\n",
    "            current_annotations[main_cat] = typ\n",
    "    \n",
    "    ann_result = []\n",
    "    for c in [\n",
    "        ['ActionName', ['File', 'Network']], \n",
    "        ['Capability', ['infection_propagation', 'command_and_control']]\n",
    "    ]:\n",
    "        category = c[0]\n",
    "        sub_cats = c[1]\n",
    "        if category in current_annotations.keys():\n",
    "            sub_cat = current_annotations[category]\n",
    "            if sub_cat in sub_cats:\n",
    "                ann_result.append(sub_cat)\n",
    "            else:\n",
    "                ann_result.append(\"Other\")\n",
    "        else:\n",
    "            ann_result.append(\"-\")\n",
    "            \n",
    "    return ann_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_relation(t, tokens, rels, anns, include_subject=False):\n",
    "    \n",
    "    global tokens_count\n",
    "    tokens_count += 1\n",
    "    \n",
    "    relations = list(filter(lambda x : x[1] == 'Action:'+t[0] , rels))\n",
    "    if relations:\n",
    "        mod_relations = list(filter(lambda x : 'Modifier' in x[2], relations))\n",
    "        if mod_relations:\n",
    "            for mr in mod_relations:\n",
    "                relations += list(filter(lambda x : x[1] == mr[2], rels))\n",
    "    \n",
    "    if include_subject:\n",
    "        subject = list(filter(lambda x : 'Subject' in x[1] and x[2] == 'Action:'+t[0], rels))\n",
    "        if subject:\n",
    "            relations.append(subject[0])\n",
    "            \n",
    "        modifier = list(filter(lambda x : 'Modifier' in x[1] and x[2] == 'Action:'+t[0], rels))\n",
    "        if modifier:\n",
    "            relations.append(modifier[0])\n",
    "            # it never happens...\n",
    "\n",
    "    if not relations:\n",
    "        print(\"Token {} has no relation.\".format(t))\n",
    "        global relation_missing\n",
    "        relation_missing += 1\n",
    "    \n",
    "    words = [t[3]]\n",
    "    \n",
    "    for r in relations:\n",
    "        \n",
    "        if include_subject and 'Subject' in r[1]: \n",
    "            # subject -> action\n",
    "            token_id = r[1].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "                        \n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                words.insert(0, token_word[0][3])\n",
    "            \n",
    "        else:\n",
    "            # action -> object, action -> mod -> object\n",
    "            token_id = r[2].split(':')[1]\n",
    "            token_word = list(filter(lambda x : x[0] == token_id, tokens))\n",
    "            if token_word and token_word[0][3] not in words:\n",
    "                if token_word[0][3][0].isupper():\n",
    "                    words.insert(0, token_word[0][3])\n",
    "                else:\n",
    "                    words.append(token_word[0][3])\n",
    "    \n",
    "    # relation gathered and here we create the dataset\n",
    "    # id prefer it in anotehr function but whatever\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    annotation_result = process_annotation(annotations, words, t)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1\n",
    "        print(words)\n",
    "    \n",
    "    return words, annotation_result             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_words_coords(t, text, off=0):\n",
    "    indices = t[2]\n",
    "    indices = indices.split(';') if ';' in indices else [indices]\n",
    "    real_token = ''\n",
    "    indices = [[int(idx) for idx in i.split()] for i in indices]\n",
    "    for i in indices:\n",
    "        real_token += text[i[0]+off : i[1]+off] + ' '\n",
    "        \n",
    "    while '- ' in real_token:\n",
    "        real_token = real_token.replace('- ', '')\n",
    "    return real_token.strip(), (indices[0][0]+off, indices[-1][1]+off)\n",
    "\n",
    "def get_token_neighbors(token, coords, text, neighbors):\n",
    "    \n",
    "    left = (neighbors-1)/2\n",
    "    right = left\n",
    "    \n",
    "    l_idx = coords[0] - 1\n",
    "    r_idx = coords[1] + 1\n",
    "    \n",
    "    words = []\n",
    "    while left:\n",
    "        while text[l_idx].isspace():\n",
    "            l_idx -= 1\n",
    "        word = ''\n",
    "        while not text[l_idx].isspace():\n",
    "            word += text[l_idx]\n",
    "            l_idx -= 1\n",
    "        \n",
    "        word = word[::-1]\n",
    "        if word[0] == '<' or word[-1] == '>': #html tag\n",
    "            l_idx -=1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.insert(0, word)\n",
    "            left -= 1\n",
    "    \n",
    "    words.append(token)\n",
    "    while right:\n",
    "        while text[r_idx].isspace():\n",
    "            r_idx += 1\n",
    "        word = ''\n",
    "        while not text[r_idx].isspace():\n",
    "            word += text[r_idx]\n",
    "            r_idx += 1\n",
    "            \n",
    "        if word[0] == '<' or word[-1] == '>':\n",
    "            r_idx += 1\n",
    "            word = ''\n",
    "        else:\n",
    "            words.append(word)\n",
    "            right -= 1\n",
    "            \n",
    "    return words\n",
    "        \n",
    "def gather_data(words, anns, t):\n",
    "    annotations = list(filter(lambda x : x[2] == t[0], anns))\n",
    "    annotation_result = process_annotation(annotations, words, t)\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"Token {} has no annotation.\".format(t))\n",
    "        global annotation_missing\n",
    "        annotation_missing += 1\n",
    "        \n",
    "    return annotation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jells123/Documents/ENGINEER/DATA/1-Raw data/data/dev/annotations\n",
      "\n",
      ">>> Carbanak_APT_eng.ann\n",
      "21\n",
      "\n",
      ">>> Anunak_APT_against_financial_institutions.ann\n",
      "13\n",
      "\n",
      ">>> Agent.BTZ_to_ComRAT.ann\n",
      "3\n",
      "\n",
      ">>> Dragonfly_Threat_Against_Western_Energy_Suppliers.ann\n",
      "13\n",
      "\n",
      ">>> GlobalThreatIntelReport.ann\n",
      "38\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "# set_type = 'Test'\n",
    "\n",
    "data, undefined = init_data()\n",
    "\n",
    "relation_mode = True\n",
    "include_subject = True\n",
    "\n",
    "if relation_mode:\n",
    "    if include_subject:\n",
    "        label = 'text-rel-subj'\n",
    "    else:\n",
    "        label = 'text-rel'\n",
    "else:\n",
    "    label = 'text-neigh'\n",
    "    \n",
    "relation_missing = 0\n",
    "annotation_missing = 0\n",
    "tokens_count = 0\n",
    "\n",
    "rel_pattern = r'(?P<r_id>R[0-9]+)\\s+(SubjAction)\\s+(?P<r_first>[A-Za-z]+:T[0-9]+)\\s+(?P<r_second>[A-Za-z]+:T[0-9]+)'\n",
    "\n",
    "path = root + 'dev' + ann\n",
    "print(path)\n",
    "\n",
    "count_relations = 0\n",
    "for filename in os.listdir(path):\n",
    "\n",
    "    if filename.endswith('.ann'):\n",
    "        print(\"\\n>>> {}\".format(filename))\n",
    "\n",
    "        with open(path + '/' + filename) as ann_file:\n",
    "            content = ann_file.read()\n",
    "\n",
    "            # match lines with a token            \n",
    "            tokens = re.findall(token_pattern, content)\n",
    "            relations = re.findall(rel_pattern, content)\n",
    "            annotations = re.findall(ann_pattern, content)\n",
    "            \n",
    "            print(len(relations))\n",
    "            count_relations += len(relations)\n",
    "\n",
    "#             with open(path + \"/\" + filename.replace(\".ann\", \".txt\")) as txt_file:\n",
    "#                 text = txt_file.read()\n",
    "#                 text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "#                 offset = 0\n",
    "#                 for t in tokens:\n",
    "#                     if t[1] == 'Action':\n",
    "\n",
    "#                         ann_token = t[3]\n",
    "#                         tokens_count += 1\n",
    "\n",
    "#                         # WHICH TOKEN?\n",
    "#                         data['token'].append(ann_token)\n",
    "\n",
    "#                         idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "\n",
    "#                         if ann_token != idx_token:\n",
    "#                             # this fixes errors occuring in one file - indices are invalid\n",
    "#                             where_is = text.find(ann_token)\n",
    "#                             offset = where_is - int(t[2].split()[0])\n",
    "#                             idx_token, coords = get_token_words_coords(t, text, offset)\n",
    "\n",
    "#                         # NEIGHBOURS\n",
    "#                         words = get_token_neighbors(ann_token, coords, text, 9)\n",
    "#                         annotation_result = gather_data(words, annotations, t)\n",
    "#                         if not words or not annotation_result:\n",
    "#                             print(\"NEIGHBOURS error\")\n",
    "#                         else:\n",
    "#                             data['text-neigh'].append(words)\n",
    "\n",
    "#                         # BASIC RELATIONS\n",
    "#                         words, annotation_result = get_token_words_relation(t, tokens, \n",
    "#                                                                         relations, annotations,\n",
    "#                                                                        include_subject=False)\n",
    "#                         if not words or not annotation_result:\n",
    "#                             print(\"RELATIONS error\")\n",
    "#                         else:\n",
    "#                             data['text-rel'].append(words)\n",
    "\n",
    "#                         # BASIC RELATIONS + SUBJECT->ACTION RELATION\n",
    "#                         words, annotation_result = get_token_words_relation(t, tokens, \n",
    "#                                                                         relations, annotations,\n",
    "#                                                                        include_subject=True)\n",
    "#                         if not words or not annotation_result:\n",
    "#                             print(\"RELATIONS+SUBJ error\")\n",
    "#                         else:\n",
    "#                             data['text-rel-subj'].append(words)\n",
    "\n",
    "\n",
    "#                         # WHICH CLASSES?\n",
    "#                         data['ActionName'].append(annotation_result[0])\n",
    "#                         data['Capability'].append(annotation_result[1])\n",
    "\n",
    "print(count_relations)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionName 344\n",
      "Capability 344\n",
      "token 344\n",
      "text-rel-subj 344\n",
      "text-rel 344\n",
      "text-neigh 344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActionName</th>\n",
       "      <th>Capability</th>\n",
       "      <th>token</th>\n",
       "      <th>text-rel-subj</th>\n",
       "      <th>text-rel</th>\n",
       "      <th>text-neigh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Network</td>\n",
       "      <td>command_and_control</td>\n",
       "      <td>upload</td>\n",
       "      <td>Seaduke operators upload files to the command-...</td>\n",
       "      <td>upload files to the command- and-control (C&amp;C)...</td>\n",
       "      <td>to infected machines by upload ng tasks to a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>Other</td>\n",
       "      <td>retrieve</td>\n",
       "      <td>They retrieve detailed bot/system information</td>\n",
       "      <td>retrieve detailed bot/system information</td>\n",
       "      <td>contact these websites to retrieve task inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>command_and_control</td>\n",
       "      <td>update</td>\n",
       "      <td>They update bot configuration</td>\n",
       "      <td>update bot configuration</td>\n",
       "      <td>retrieve detailed bot/system information, upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Network</td>\n",
       "      <td>-</td>\n",
       "      <td>upload</td>\n",
       "      <td>They upload files</td>\n",
       "      <td>upload files</td>\n",
       "      <td>information, update bot configuration, upload ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Network</td>\n",
       "      <td>-</td>\n",
       "      <td>download</td>\n",
       "      <td>They download files</td>\n",
       "      <td>download files</td>\n",
       "      <td>bot configuration, upload files, download file...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ActionName           Capability     token  \\\n",
       "0    Network  command_and_control    upload   \n",
       "1          -                Other  retrieve   \n",
       "2          -  command_and_control    update   \n",
       "3    Network                    -    upload   \n",
       "4    Network                    -  download   \n",
       "\n",
       "                                       text-rel-subj  \\\n",
       "0  Seaduke operators upload files to the command-...   \n",
       "1      They retrieve detailed bot/system information   \n",
       "2                      They update bot configuration   \n",
       "3                                  They upload files   \n",
       "4                                They download files   \n",
       "\n",
       "                                            text-rel  \\\n",
       "0  upload files to the command- and-control (C&C)...   \n",
       "1           retrieve detailed bot/system information   \n",
       "2                           update bot configuration   \n",
       "3                                       upload files   \n",
       "4                                     download files   \n",
       "\n",
       "                                          text-neigh  \n",
       "0       to infected machines by upload ng tasks to a  \n",
       "1  contact these websites to retrieve task inform...  \n",
       "2  retrieve detailed bot/system information, upda...  \n",
       "3  information, update bot configuration, upload ...  \n",
       "4  bot configuration, upload files, download file...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "[print(key, len(data[key])) for key in data.keys()]\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "    key : data[key] for key in data.keys()\n",
    "})\n",
    "\n",
    "# dataset['text-rel-lengths'] = dataset['text-rel'].apply(lambda x : len((' '.join(x)).split()))\n",
    "# dataset['text-rel-subj-lengths'] = dataset['text-rel-subj'].apply(lambda x : len((' '.join(x)).split()))\n",
    "\n",
    "# print(np.mean(dataset['text-rel-lengths']))\n",
    "# print(np.median(dataset['text-rel-lengths']))\n",
    "\n",
    "# print(np.mean(dataset['text-rel-subj-lengths']))\n",
    "# print(np.median(dataset['text-rel-subj-lengths']))\n",
    "\n",
    "for column in ['text-rel', 'text-neigh', 'text-rel-subj']:\n",
    "    dataset[column] = dataset[column].apply(' '.join)\n",
    "    dataset[column] = dataset[column].apply(lambda x : re.sub(r\"[“”\\[\\]]\", \"\", x))  \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Test__________ \n",
      "\n",
      "-          229\n",
      "Other       51\n",
      "File        45\n",
      "Network     19\n",
      "Name: ActionName, dtype: int64 \n",
      "\n",
      "Other                    214\n",
      "-                         65\n",
      "command_and_control       46\n",
      "infection_propagation     19\n",
      "Name: Capability, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"_\" * 10 + set_type + \"_\" * 10, '\\n')\n",
    "dataset.to_csv('All-{}.csv'.format(set_type))\n",
    "print(dataset.ActionName.value_counts(), \"\\n\")\n",
    "print(dataset.Capability.value_counts(), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
