{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "class HierarchicalModel:\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "        self.train_results = None\n",
    "        self.test_results = None\n",
    "        \n",
    "        self.main_class_predictor = lambda x: 1 if x[1] > x[0] else 1\n",
    "        self.sub_class_predictor = None\n",
    "    \n",
    "    def loadData(self, path, main_cats_list):\n",
    "        dataframes = {}\n",
    "\n",
    "        print(\"Loading data...\\n\")\n",
    "        ls = os.listdir(path)\n",
    "\n",
    "        for file in ls:\n",
    "            if '.csv' in file and 'old' not in file:\n",
    "\n",
    "                print(path + file)\n",
    "                df = pd.read_csv(path + file, encoding='utf-8')\n",
    "\n",
    "                for cname in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"label\"]:\n",
    "                    if cname in df.columns:\n",
    "                        df.rename({cname:\"a\"}, axis=\"columns\", inplace=True)\n",
    "                        df.drop([\"a\"], axis=1, inplace=True)\n",
    "                df_type = file.split('-')[1]\n",
    "\n",
    "                dataframes[df_type] = df\n",
    "\n",
    "        self.train = dataframes['Train']\n",
    "        self.test = dataframes['Test']\n",
    "        \n",
    "        categories, mapper = {}, {}\n",
    "        for mc in main_cats_list:\n",
    "            categories[mc] = list(sorted(set(filter(lambda x : x != '-', self.train[mc]))))\n",
    "            mapper[mc] = {}\n",
    "            for cat in np.unique(self.train[mc]):\n",
    "                mapper[mc][cat] = np.unique(self.train[self.train[mc] == cat][\"{}_num\".format(mc)])[0]\n",
    "        \n",
    "        self.mapper = mapper\n",
    "        self.categories = categories\n",
    "        self.main_cats = main_cats_list\n",
    "        \n",
    "        print(\"\\nLoaded categories:\\n\")\n",
    "        print(categories)\n",
    "\n",
    "    def predictMainClass(self, data_c, pipeline, report=False, undefined=\"-\"):\n",
    "        \n",
    "#         if not self.train_results:\n",
    "        self.train_results = pd.DataFrame()\n",
    "#         if not self.test_results:\n",
    "        self.test_results = pd.DataFrame()\n",
    "        \n",
    "        if report:\n",
    "            print(\"\\n > > > MAIN CLASS CLASSIFICATION > > >\\n\")\n",
    "        else:\n",
    "            print(\"\\nClassifying main classes...\")\n",
    "            \n",
    "        for main_cat in self.main_cats:\n",
    "            \n",
    "            if report:\n",
    "                print(\"-\" * 52)\n",
    "\n",
    "            train_labels = self.train[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "            test_labels = self.test[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "\n",
    "            # train pipeline\n",
    "            pipeline.fit(self.train[data_c], train_labels)\n",
    "\n",
    "            # store train set results\n",
    "            self.train_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "            self.train_results['{}-def_true'.format(main_cat)] = train_labels\n",
    "\n",
    "            train_pred = pipeline.predict(self.train[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TRAIN results:\".format(main_cat))\n",
    "                print(classification_report(train_labels, train_pred))\n",
    "\n",
    "                tpp = pipeline.predict_proba(self.train[data_c])[:, 1]\n",
    "                print(\"ROC AUC -> {}\".format(roc_auc_score(train_labels, tpp)))\n",
    "                print(\"Acc: {}\".format(accuracy_score(train_labels, train_pred)))\n",
    "\n",
    "            # store test set results\n",
    "            self.test_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "            self.test_results['{}-def_true'.format(main_cat)] = test_labels\n",
    "\n",
    "            test_pred = pipeline.predict(self.test[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TEST results:\".format(main_cat))\n",
    "                print(classification_report(test_labels, test_pred))\n",
    "                \n",
    "                tpp = pipeline.predict_proba(self.test[data_c])[:, 1]\n",
    "                print(\"ROC AUC -> {}\".format(roc_auc_score(test_labels, tpp)))\n",
    "                print(\"Acc: {}\".format(accuracy_score(test_labels, test_pred)))\n",
    "\n",
    "            # prepare columns to store our final predictions\n",
    "            self.train_results['{}_PREDICTION'.format(main_cat)] = '?'    \n",
    "            self.test_results['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            # additional column for test-set\n",
    "            # replace '?' with '-' according to pipeline's prediction\n",
    "            # - if it's very certain about answering NO, it won't be taken into consideration\n",
    "            # in next level of classification\n",
    "\n",
    "            self.test['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            for i in range(self.test_results.shape[0]):\n",
    "                row = self.test_results.loc[i]\n",
    "                def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                pred = self.main_class_predictor(def_pred)\n",
    "                if pred == 0:\n",
    "                    self.test.at[i, \"{}_PREDICTION\".format(main_cat)] = undefined\n",
    "                    \n",
    "        if not report:\n",
    "            print(\"Done!\")\n",
    "                    \n",
    "    def predictSubClass(self, data_c, pipeline, report=False, undefined=\"-\"):\n",
    "\n",
    "        print(\"\\nClassifying subclasses [binary classifiers!]...\")\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            print('\\n' + main_cat + \":\")\n",
    "            cats = self.categories[main_cat]\n",
    "\n",
    "            for cat in cats:\n",
    "                print(\"- \" + cat)\n",
    "\n",
    "                # subset of train set where main_cat is defined\n",
    "                train_subset = self.train.loc[self.train[main_cat] != undefined]\n",
    "                # subset of test set where main_cat is defined, according to previous classification stage\n",
    "                test_subset = self.test.loc[self.test[\"{}_PREDICTION\".format(main_cat)] != undefined] # !!!\n",
    "\n",
    "                # prepare binary labels: 1 for this class, 0 for any other\n",
    "                train_labels = train_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "                test_labels = test_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "\n",
    "                # train pipeline\n",
    "                pipeline.fit(train_subset[data_c], train_labels)\n",
    "\n",
    "                pred_label = '{}_prediction'.format(cat)\n",
    "                true_label = '{}_true'.format(cat)\n",
    "\n",
    "                # store results \n",
    "                self.train_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "                self.train_results[true_label] = train_labels\n",
    "\n",
    "                self.test_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "                self.test_results[true_label] = test_labels\n",
    "\n",
    "                # fix NaN issues...\n",
    "                for df in [self.train_results, self.test_results]:\n",
    "                    df[true_label] = df[true_label].map(lambda x : '-' if math.isnan(x) else int(x))\n",
    "          \n",
    "        print(\"\\nMaking decision...\")\n",
    "        \n",
    "        for df in [self.train_results, self.test_results]:\n",
    "            # iterate over rows\n",
    "            for i in range(df.shape[0]):\n",
    "                row = df.loc[i]\n",
    "\n",
    "                for main_cat in self.main_cats:\n",
    "                    # get main class prediction\n",
    "                    def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                    # make decision :)\n",
    "                    pred = self.main_class_predictor(def_pred)\n",
    "                    # get truth (mostly for debugging)\n",
    "                    truth = row['{}-def_true'.format(main_cat)]\n",
    "\n",
    "                    if pred:\n",
    "                        # if predicted as defined, get sub classes\n",
    "                        cats = self.categories[main_cat]\n",
    "\n",
    "                        positive_scores = []\n",
    "                        negative_scores = []\n",
    "\n",
    "                        # gather negative and positive votes\n",
    "                        for cat in cats:\n",
    "                            cat_prob = row['{}_prediction'.format(cat)]\n",
    "                            negative_scores.append(cat_prob[0] * def_pred[0])\n",
    "                            positive_scores.append(cat_prob[1] * def_pred[1])\n",
    "\n",
    "                        # get best scores for negative and positive answer\n",
    "                        pos_idx = np.argmax(positive_scores)\n",
    "                        neg_idx = np.argmax(negative_scores)\n",
    "\n",
    "                        if positive_scores[pos_idx] > negative_scores[neg_idx]:\n",
    "                            best_class = cats[pos_idx]   \n",
    "                        else:\n",
    "                            best_class = '-'\n",
    "\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = best_class\n",
    "                    else:\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = \"-\"\n",
    "                        \n",
    "        print(\"Done!\")\n",
    "     \n",
    "    '''\n",
    "    tpp = pipeline.predict_proba(self.train[data_c])\n",
    "\n",
    "    print(\"ROC AUC -> {}\".format(roc_auc_score(train_labels, tpp)))\n",
    "    print(\"Acc: {}\".format(accuracy_score(train_labels, train_pred)))\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    def setPredictedLabelsNumbers(self):\n",
    "        for main_cat in self.main_cats:\n",
    "            self.train[\"{}-PREDICTION_num\".format(main_cat)] = self.train_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "            self.test[\"{}-PREDICTION_num\".format(main_cat)] = self.test_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "\n",
    "  \n",
    "    def showResults(self):\n",
    "        self.setPredictedLabelsNumbers()\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            print()\n",
    "            print(\"-\"*20 + \" \" + main_cat + \" \" + \"-\"*20)\n",
    "            print(self.mapper[main_cat])\n",
    "            \n",
    "            print(\"\\n>>> TRAIN:\")\n",
    "            y_true = self.train['{}_num'.format(main_cat)]\n",
    "            y_pred = self.train['{}-PREDICTION_num'.format(main_cat)]\n",
    "            \n",
    "            print(classification_report(y_true, y_pred),\n",
    "#                  target_names=self.mapper[main_cat]\n",
    "                 )         \n",
    "            \n",
    "            print(\"\\n>>> TEST:\")\n",
    "            y_true = self.test['{}_num'.format(main_cat)]\n",
    "            y_pred = self.test['{}-PREDICTION_num'.format(main_cat)]\n",
    "            \n",
    "            print(classification_report(y_true, y_pred))\n",
    "\n",
    "    def getResultsDict(self):\n",
    "        self.setPredictedLabelsNumbers()\n",
    "        \n",
    "        result = {\n",
    "            \"Train\" : dict(),\n",
    "            \"Test\" : dict()\n",
    "        }\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            \n",
    "            y_true = self.train['{}_num'.format(main_cat)]\n",
    "            y_pred = self.train['{}-PREDICTION_num'.format(main_cat)]\n",
    "            \n",
    "            c_train = classification_report(y_true, y_pred, output_dict=True)\n",
    "            acc = accuracy_score(y_true, y_pred) \n",
    "            \n",
    "            result[\"Train\"][main_cat] = c_train\n",
    "            result[\"Train\"]['Accuracy'] = acc\n",
    "            \n",
    "            y_true = self.test['{}_num'.format(main_cat)]\n",
    "            y_pred = self.test['{}-PREDICTION_num'.format(main_cat)]\n",
    "            \n",
    "            c_test = classification_report(y_true, y_pred, output_dict=True)\n",
    "            acc = accuracy_score(y_true, y_pred) \n",
    "            \n",
    "            result[\"Test\"][main_cat] = c_test\n",
    "            result[\"Test\"]['Accuracy'] = acc\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare simple pipeline\n",
    "'''\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([          \n",
    "    ('vectorizer',\n",
    "     TfidfVectorizer(\n",
    "         preprocessor=dummy, \n",
    "         ngram_range=(1, 1),\n",
    "         analyzer='word',\n",
    "         binary=False\n",
    "     )\n",
    "#      TfidfVectorizer(\n",
    "#          preprocessor=dummy, \n",
    "#          ngram_range=(1, 3),\n",
    "#          analyzer='word',\n",
    "#          max_df=0.15,\n",
    "#          max_features=3000\n",
    "#      )\n",
    "    ),\n",
    "    \n",
    "    ('clf', \n",
    "     LogisticRegression(\n",
    "         class_weight='balanced', solver='liblinear'\n",
    "     )\n",
    "#      SVC(probability=True)\n",
    "#      MultinomialNB()\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Train-P.csv\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Test-P.csv\n",
      "\n",
      "Loaded categories:\n",
      "\n",
      "{'ActionName': ['File', 'Network', 'Other'], 'Capability': ['Other', 'command_and_control', 'infection_propagation']}\n",
      "\n",
      "Classifying main classes...\n",
      "Done!\n",
      "\n",
      "Classifying subclasses [binary classifiers!]...\n",
      "\n",
      "ActionName:\n",
      "- File\n",
      "- Network\n",
      "- Other\n",
      "\n",
      "Capability:\n",
      "- Other\n",
      "- command_and_control\n",
      "- infection_propagation\n",
      "\n",
      "Making decision...\n",
      "Done!\n",
      "\n",
      "~~~~~Results for column 'text-rel-tokens'+'text-rel-tokens'~~~~~\n",
      "\n",
      "{   'Test': {   'Accuracy': 0.6794055201698513,\n",
      "                'ActionName': {   'weighted avg': {   'f1-score': 0.7430427307887767,\n",
      "                                                      'precision': 0.7539926215374504,\n",
      "                                                      'recall': 0.7367303609341825,\n",
      "                                                      'support': 471}},\n",
      "                'Capability': {   'weighted avg': {   'f1-score': 0.6913193485436308,\n",
      "                                                      'precision': 0.7465447374018309,\n",
      "                                                      'recall': 0.6794055201698513,\n",
      "                                                      'support': 471}}},\n",
      "    'Train': {   'Accuracy': 0.8375149342891278,\n",
      "                 'ActionName': {   'weighted avg': {   'f1-score': 0.8461667381142545,\n",
      "                                                       'precision': 0.8487894321132853,\n",
      "                                                       'recall': 0.8452807646356033,\n",
      "                                                       'support': 3348}},\n",
      "                 'Capability': {   'weighted avg': {   'f1-score': 0.841397488814622,\n",
      "                                                       'precision': 0.8651148095119287,\n",
      "                                                       'recall': 0.8375149342891278,\n",
      "                                                       'support': 3348}}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "c1 = 'text-rel-tokens'\n",
    "c2 = c1\n",
    "\n",
    "hm = HierarchicalModel()\n",
    "hm.loadData(os.getcwd() + \"/Dataframes/All/Processed/\", [\"ActionName\", \"Capability\"])\n",
    "# hm.main_class_predictor = lambda predictions: 0 if predictions[0] - predictions[1] > 0.5 else 1\n",
    "hm.main_class_predictor = lambda predictions: 0 if predictions[0] > predictions[1] else 1\n",
    "\n",
    "for column in ['text-rel-tokens', 'text-rel-subj-tokens', 'text-neigh-tokens', 'text-rel-word-tokenized']:\n",
    "    c1 = column\n",
    "    c2 = column\n",
    "    if 'text-rel-tokens' not in c1:\n",
    "        continue\n",
    "\n",
    "    hm.predictMainClass(c1, pipeline, report=False)\n",
    "    hm.predictSubClass(c2, pipeline)\n",
    "\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    result = hm.getResultsDict()\n",
    "    for dataset_type in [\"Train\", \"Test\"]:\n",
    "        subdict = result[dataset_type]\n",
    "        for cat in subdict.keys():\n",
    "            if 'Accuracy' in cat:\n",
    "                continue\n",
    "            cat_dict = subdict[cat]\n",
    "            keys = list(cat_dict.keys()).copy()\n",
    "            for label in keys:\n",
    "                if 'weighted' not in label:\n",
    "                    cat_dict.pop(label)\n",
    "\n",
    "    print(\"\\n\" + \"~\" * 5 + \"Results for column '{}'+'{}'\".format(c1, c2) + \"~\" * 5 + \"\\n\")\n",
    "    pp.pprint(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
