{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class HierarchicalModel:\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "        self.train_results = None\n",
    "        self.test_results = None\n",
    "        \n",
    "        self.main_class_predictor = lambda x: 1 if x[1] > x[0] else 1\n",
    "        self.sub_class_predictor = None\n",
    "    \n",
    "    def loadData(self, path, main_cats_list):\n",
    "        dataframes = {}\n",
    "\n",
    "        print(\"Loading data...\\n\")\n",
    "        ls = os.listdir(path)\n",
    "\n",
    "        for file in ls:\n",
    "            if '.csv' in file and 'old' not in file:\n",
    "\n",
    "                print(path + file)\n",
    "                df = pd.read_csv(path + file, encoding='utf-8')\n",
    "\n",
    "                for cname in [\"Unnamed: 0\", \"Unnamed: 0.1\", \"label\"]:\n",
    "                    if cname in df.columns:\n",
    "                        df.rename({cname:\"a\"}, axis=\"columns\", inplace=True)\n",
    "                        df.drop([\"a\"], axis=1, inplace=True)\n",
    "                df_type = file.split('-')[1]\n",
    "\n",
    "                dataframes[df_type] = df\n",
    "\n",
    "        self.train = dataframes['Train']\n",
    "        self.test = dataframes['Test']\n",
    "        \n",
    "        categories, mapper = {}, {}\n",
    "        for mc in main_cats_list:\n",
    "            categories[mc] = list(sorted(set(filter(lambda x : x != '-', self.train[mc]))))\n",
    "            mapper[mc] = {}\n",
    "            for cat in np.unique(self.train[mc]):\n",
    "                mapper[mc][cat] = np.unique(self.train[self.train[mc] == cat][\"{}_num\".format(mc)])[0]\n",
    "        \n",
    "        self.mapper = mapper\n",
    "        self.categories = categories\n",
    "        self.main_cats = main_cats_list\n",
    "        \n",
    "        print(\"\\nLoaded categories:\\n\")\n",
    "        print(categories)\n",
    "\n",
    "    def predictMainClass(self, data_c, pipeline, report=False, undefined=\"-\"):\n",
    "        \n",
    "        if not self.train_results:\n",
    "            self.train_results = pd.DataFrame()\n",
    "        if not self.test_results:\n",
    "            self.test_results = pd.DataFrame()\n",
    "        \n",
    "        if report:\n",
    "            print(\"\\n > > > MAIN CLASS CLASSIFICATION > > >\\n\")\n",
    "        else:\n",
    "            print(\"\\nClassifying main classes...\")\n",
    "            \n",
    "        for main_cat in self.main_cats:\n",
    "            \n",
    "            if report:\n",
    "                print(\"-\" * 52)\n",
    "\n",
    "            train_labels = self.train[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "            test_labels = self.test[main_cat].map(lambda x : 0 if x == undefined else 1)\n",
    "\n",
    "            # train pipeline\n",
    "            pipeline.fit(self.train[data_c], train_labels)\n",
    "\n",
    "            # store train set results\n",
    "            self.train_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "            self.train_results['{}-def_true'.format(main_cat)] = train_labels\n",
    "\n",
    "            train_pred = pipeline.predict(self.train[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TRAIN results:\".format(main_cat))\n",
    "                print(classification_report(train_labels, train_pred))\n",
    "\n",
    "            # store test set results\n",
    "            self.test_results['{}-def_prediction'.format(main_cat)] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "            self.test_results['{}-def_true'.format(main_cat)] = test_labels\n",
    "\n",
    "            test_pred = pipeline.predict(self.test[data_c])\n",
    "            if report:\n",
    "                print(\"\\n{} -> TEST results:\".format(main_cat))\n",
    "                print(classification_report(test_labels, test_pred))\n",
    "\n",
    "            # prepare columns to store our final predictions\n",
    "            self.train_results['{}_PREDICTION'.format(main_cat)] = '?'    \n",
    "            self.test_results['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            # additional column for test-set\n",
    "            # replace '?' with '-' according to pipeline's prediction\n",
    "            # - if it's very certain about answering NO, it won't be taken into consideration\n",
    "            # in next level of classification\n",
    "\n",
    "            self.test['{}_PREDICTION'.format(main_cat)] = '?'\n",
    "\n",
    "            for i in range(self.test_results.shape[0]):\n",
    "                row = self.test_results.loc[i]\n",
    "                def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                pred = self.main_class_predictor(def_pred)\n",
    "                if pred == 0:\n",
    "                    self.test.at[i, \"{}_PREDICTION\".format(main_cat)] = undefined\n",
    "                    \n",
    "        if not report:\n",
    "            print(\"Done!\")\n",
    "                    \n",
    "    def predictSubClass(self, data_c, pipeline, undefined=\"-\"):\n",
    "\n",
    "        print(\"\\nClassifying subclasses [binary classifiers!]...\")\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            print('\\n' + main_cat + \":\")\n",
    "            cats = self.categories[main_cat]\n",
    "\n",
    "            for cat in cats:\n",
    "                print(\"- \" + cat)\n",
    "\n",
    "                # subset of train set where main_cat is defined\n",
    "                train_subset = self.train.loc[self.train[main_cat] != undefined]\n",
    "                # subset of test set where main_cat is defined, according to previous classification stage\n",
    "                test_subset = self.test.loc[self.test[\"{}_PREDICTION\".format(main_cat)] != undefined] # !!!\n",
    "\n",
    "                # prepare binary labels: 1 for this class, 0 for any other\n",
    "                train_labels = train_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "                test_labels = test_subset[main_cat].map(lambda x : 0 if x != cat else 1)\n",
    "\n",
    "                # train pipeline\n",
    "                pipeline.fit(train_subset[data_c], train_labels)\n",
    "\n",
    "                pred_label = '{}_prediction'.format(cat)\n",
    "                true_label = '{}_true'.format(cat)\n",
    "\n",
    "                # store results \n",
    "                self.train_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.train[data_c])]\n",
    "                self.train_results[true_label] = train_labels\n",
    "\n",
    "                self.test_results[pred_label] = [list(p) for p in pipeline.predict_proba(self.test[data_c])]\n",
    "                self.test_results[true_label] = test_labels\n",
    "\n",
    "                # fix NaN issues...\n",
    "                for df in [self.train_results, self.test_results]:\n",
    "                    df[true_label] = df[true_label].map(lambda x : '-' if math.isnan(x) else int(x))\n",
    "          \n",
    "        print(\"\\nMaking decision...\")\n",
    "        \n",
    "        for df in [self.train_results, self.test_results]:\n",
    "            # iterate over rows\n",
    "            for i in range(df.shape[0]):\n",
    "                row = df.loc[i]\n",
    "\n",
    "                for main_cat in self.main_cats:\n",
    "                    # get main class prediction\n",
    "                    def_pred = row['{}-def_prediction'.format(main_cat)]\n",
    "                    # make decision :)\n",
    "                    pred = self.main_class_predictor(def_pred)\n",
    "                    # get truth (mostly for debugging)\n",
    "                    truth = row['{}-def_true'.format(main_cat)]\n",
    "\n",
    "                    if pred:\n",
    "                        # if predicted as defined, get sub classes\n",
    "                        cats = self.categories[main_cat]\n",
    "\n",
    "                        positive_scores = []\n",
    "                        negative_scores = []\n",
    "\n",
    "                        # gather negative and positive votes\n",
    "                        for cat in cats:\n",
    "                            cat_prob = row['{}_prediction'.format(cat)]\n",
    "                            negative_scores.append(cat_prob[0] * def_pred[0])\n",
    "                            positive_scores.append(cat_prob[1] * def_pred[1])\n",
    "\n",
    "                        # get best scores for negative and positive answer\n",
    "                        pos_idx = np.argmax(positive_scores)\n",
    "                        neg_idx = np.argmax(negative_scores)\n",
    "\n",
    "                        if positive_scores[pos_idx] > negative_scores[neg_idx]:\n",
    "                            best_class = cats[pos_idx]   \n",
    "                        else:\n",
    "                            best_class = '-'\n",
    "\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = best_class\n",
    "                    else:\n",
    "                        df.at[i, \"{}_PREDICTION\".format(main_cat)] = \"-\"\n",
    "                        \n",
    "        print(\"Done!\")\n",
    "        \n",
    "    def showResults(self):\n",
    "        \n",
    "        for main_cat in self.main_cats:\n",
    "            self.train[\"{}-PREDICTION_num\".format(main_cat)] = self.train_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "            self.test[\"{}-PREDICTION_num\".format(main_cat)] = self.test_results[\"{}_PREDICTION\".format(main_cat)].map(self.mapper[main_cat])\n",
    "\n",
    "        for main_cat in self.main_cats:\n",
    "            print()\n",
    "            print(\"-\"*20 + \" \" + main_cat + \" \" + \"-\"*20)\n",
    "            print(self.mapper[main_cat])\n",
    "            \n",
    "            print(\"\\n>>> TRAIN:\")\n",
    "            print(classification_report(self.train['{}_num'.format(main_cat)], self.train['{}-PREDICTION_num'.format(main_cat)]),\n",
    "#                  target_names=self.mapper[main_cat]\n",
    "                 )\n",
    "            \n",
    "            print(\"\\n>>> TEST:\")\n",
    "            print(classification_report(self.test['{}_num'.format(main_cat)], self.test['{}-PREDICTION_num'.format(main_cat)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare simple pipeline\n",
    "'''\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([          \n",
    "    ('vectorizer',\n",
    "     CountVectorizer(\n",
    "         preprocessor=dummy, \n",
    "         ngram_range=(1, 1),\n",
    "         analyzer='word',\n",
    "         binary=False\n",
    "     )\n",
    "#      TfidfVectorizer(\n",
    "#          preprocessor=dummy, \n",
    "#          ngram_range=(1, 3),\n",
    "#          analyzer='word',\n",
    "#          max_df=0.15,\n",
    "#          max_features=3000\n",
    "#      )\n",
    "    ),\n",
    "    \n",
    "    ('clf', \n",
    "#      LogisticRegression(\n",
    "#          class_weight='balanced',\n",
    "#          solver='liblinear'\n",
    "#      )\n",
    "     MultinomialNB()\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Train-P.csv\n",
      "/home/jells123/Documents/ENGINEER/Secure-NLP/Dataframes/All/Processed/All-Test-P.csv\n",
      "\n",
      "Loaded categories:\n",
      "\n",
      "{'ActionName': ['File', 'Network', 'Other'], 'Capability': ['command_and_control', 'infection_propagation', 'other']}\n",
      "\n",
      "Classifying main classes...\n",
      "Done!\n",
      "\n",
      "Classifying subclasses [binary classifiers!]...\n",
      "\n",
      "ActionName:\n",
      "- File\n",
      "- Network\n",
      "- Other\n",
      "\n",
      "Capability:\n",
      "- command_and_control\n",
      "- infection_propagation\n",
      "- other\n",
      "\n",
      "Making decision...\n",
      "Done!\n",
      "\n",
      "-------------------- ActionName --------------------\n",
      "{'-': 0, 'File': 1, 'Network': 2, 'Other': 3}\n",
      "\n",
      ">>> TRAIN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      2194\n",
      "           1       0.76      0.78      0.77       325\n",
      "           2       0.78      0.67      0.72       373\n",
      "           3       0.79      0.81      0.80       456\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      3348\n",
      "   macro avg       0.81      0.79      0.80      3348\n",
      "weighted avg       0.86      0.86      0.86      3348\n",
      "\n",
      "\n",
      ">>> TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       157\n",
      "           1       0.72      0.56      0.63        32\n",
      "           2       0.52      0.57      0.54        23\n",
      "           3       0.58      0.60      0.59        25\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       237\n",
      "   macro avg       0.66      0.65      0.65       237\n",
      "weighted avg       0.76      0.76      0.76       237\n",
      "\n",
      "\n",
      "-------------------- Capability --------------------\n",
      "{'-': 0, 'command_and_control': 1, 'infection_propagation': 2, 'other': 3}\n",
      "\n",
      ">>> TRAIN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.70      0.73       531\n",
      "           1       0.84      0.85      0.85       580\n",
      "           2       0.90      0.81      0.86       525\n",
      "           3       0.83      0.87      0.85      1712\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      3348\n",
      "   macro avg       0.83      0.81      0.82      3348\n",
      "weighted avg       0.83      0.83      0.83      3348\n",
      "\n",
      "\n",
      ">>> TEST:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.30      0.39        47\n",
      "           1       0.56      0.77      0.65        31\n",
      "           2       0.46      0.65      0.54        17\n",
      "           3       0.74      0.76      0.75       142\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       237\n",
      "   macro avg       0.58      0.62      0.58       237\n",
      "weighted avg       0.66      0.66      0.65       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hm = HierarchicalModel()\n",
    "hm.loadData(os.getcwd() + \"/Dataframes/All/Processed/\", [\"ActionName\", \"Capability\"])\n",
    "hm.main_class_predictor = lambda predictions: 0 if predictions[0] - predictions[1] > 0.5 else 1\n",
    "hm.predictMainClass('text-rel-tokens', pipeline, report=False)\n",
    "hm.predictSubClass('text-rel-tokens', pipeline)\n",
    "hm.showResults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
